\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} 
\usepackage[]{amssymb} 
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{comment}
\usepackage[dvipsnames]{xcolor}
\usepackage[thinc]{esdiff}
\newtheorem*{theorem}{Theorem}

\title{Optimization Algorithms: HW0}
\author{Lo Chun, Chou \\ R13922136}
\date\today

\begin{document}
\setlength{\parindent}{0pt}
\maketitle 

\section*{1}
\subsection*{(1)}

To show that the optimization problem defining $w^\natural$ is convex, 
we need to show that both the objective function and the constraint set are convex.
\bigskip

Claim: The objective function 
$g (w) := \frac{1}{2n} \sum_{i = 1}^n ( y_i - \langle x_i, w \rangle )^2$ is convex,
and the constraint set $\mathbb{R}^d$ is also convex.

\bigskip
To prove that $g(w)$ is convex, we would use the theorem that:

\begin{theorem}
    \footnote{S. Boyd and L. Vandenberghe, \textit{Convex Optimization}, Cambridge University Press, 2004, pp. 71.}
    Assume that a function $f$ is twice differentiable, then $f$ is convex $\Leftrightarrow$ 
    $\mathrm{dom} f$ is convex and its Hessian is positive semidefinite.
\end{theorem}

To check that if $g(w)$ is twice differentiable, we first convert the original definition into a matrix-vector form, by letting:

\begin{equation*}
    X = \begin{bmatrix}
        x_1^\intercal  \\
        x_2^\intercal  \\
        \vdots \\
        x_n^\intercal 
    \end{bmatrix} \in \mathbb{R}^{n \times d}
    \quad \text{and} \quad
    y = \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{bmatrix} \in \mathbb{R}^n
    \quad \text{and} \quad
    w = \begin{bmatrix}
        w_1 \\
        w_2 \\
        \vdots \\
        w_d
    \end{bmatrix} \in \mathbb{R}^d
\end{equation*}

Then, we'll get:

\begin{equation*}
    \begin{split}
    g(w) 
        &= \frac{1}{2n} \sum_{i = 1}^n ( y_i - x_i^\intercal  w)^2 \\
        &= \frac{1}{2n} \sum_{i = 1}^n\left[y_i^2 - 2y_ix_i^\intercal w + (x_i^\intercal w)^2 \right]\\
        &= \frac{1}{2n} (y^\intercal y - 2y^\intercal Xw + (Xw)^\intercal Xw)\\
        &= \frac{1}{2n} (y^\intercal y - 2w^\intercal X^\intercal y + w^\intercal X^\intercal Xw)
    \end{split}
\end{equation*}

Differentiate w.r.t. $w$:

\begin{align*}
    \nabla g(w) 
        &= \diffp{}{w} \left[\frac{1}{2n} (y^\intercal y - 2w^\intercal X^\intercal y + w^\intercal X^\intercal Xw) \right]\\
        &= \frac{1}{2n} \left[0 - 2X^\intercal y + 2X^\intercal Xw\right] \\
        &= - \frac{1}{n} X^\intercal y + \frac{1}{n} X^\intercal Xw \tag{1}
\end{align*} 

Then the second derivative:

\begin{align*}
    \nabla^2 g(w) 
        &= \diffp{}{w} \left[- \frac{1}{n} X^\intercal y + \frac{1}{n} X^\intercal Xw \right]\\
        &= \frac{1}{n} X^\intercal X \tag{2}
\end{align*}

Since $\frac{1}{n} X^\intercal X$ does not depend on $w$, it is a constant matrix, 
and therefore the second derivative exists at each point in $\mathrm{dom} f$.
We can now check the conditions of the theorem.
\bigskip

The domain of $g(w)$ is $\mathbb{R}^d$, which is convex.
\footnote{S. Boyd and L. Vandenberghe, \textit{Convex Optimization}, Cambridge University Press, 2004, pp. 27.}
\bigskip

For any $v \in \mathbb{R}^d$, we have:

\begin{equation*}
    \begin{split}
        v^\intercal  \nabla^2 g(w) v 
            &= v^\intercal  \frac{1}{n} X^\intercal X v \\
            &= \frac{1}{n} (Xv)^\intercal  Xv \\
            &= \frac{1}{n} \|Xv\|_2^2 \ge 0
    \end{split}
\end{equation*}

Thus, the Hessian of $g(w)$ is positive semidefinite, and $g(w)$ is convex. 
\bigskip

Finally, the constraint set $\mathbb{R}^d$ is also convex, as shown above,
we can conclude that the optimization problem defining $w^\natural$ is convex. $\square$

\subsection*{(2)}

For $t=1$, we have:

\begin{equation*}
    w_2 = w_1 - \left( \nabla^2 g ( w_1 ) \right)^{-1} \nabla g ( w_1 ), 
    \qquad \text{where } w_1 = 0 \in \mathbb{R}^d \tag{*}
\end{equation*}
\newpage

To get $w_2$, we need to calculate $\nabla g ( w_1 ), \ \nabla^2 g ( w_1 )$,
from $(1)$ in the previous question, we have:

\begin{equation*}
    \begin{split}
        \nabla g ( w_1 ) 
            &= - \frac{1}{n} X^\intercal y + \frac{1}{n} X^\intercal Xw_1 \\
            &= - \frac{1}{n} X^\intercal y
    \end{split}
\end{equation*}

And from $(2)$ in the previous question, we have:

\begin{equation*}
    \nabla^2 g ( w_1 ) = \frac{1}{n} X^\intercal X
\end{equation*}
    
Plugging back into $(*)$, we get:

\begin{equation*}
    \begin{split}
        w_2 
            &= w_1 - \left( \nabla^2 g ( w_1 ) \right)^{-1} \nabla g ( w_1 ) \\
            &= 0 - \left( \frac{1}{n} X^\intercal X \right)^{-1} \left( - \frac{1}{n} X^\intercal y \right) \\
            &= 0 + n (X^\intercal X)^{-1} \frac{1}{n} X^\intercal y \\
            &= (X^\intercal X)^{-1} X^\intercal y 
    \end{split} \tag{1}  
\end{equation*}

To show that $w_2 = w^\natural$, observe that $\nabla g(w^\natural) = 0$, using $(1)$ in the previous question, we have: 

\begin{equation*}
    \begin{split}
        \nabla g(w^\natural) 
            &= - \frac{1}{n} X^\intercal y + \frac{1}{n} X^\intercal Xw^\natural \\
            &= - \frac{1}{n} X^\intercal y + \frac{1}{n} X^\intercal Xw^\natural = 0
    \end{split}
\end{equation*}

Reorder and simplify the terms, we get:

\begin{equation*}
    X^\intercal Xw^\natural = X^\intercal y
\end{equation*}

Since $rank(X) = d$, $rank(X^\intercal X) = rank(X) = d$, 
$X^\intercal X \in \mathbb{R}^{d \times d}$ is of full rank
and therefore invertible (and positive definite).

Thus, we can write:

\begin{equation*}
    w^\natural = (X^\intercal X)^{-1} X^\intercal y
\end{equation*}

which is the same as $w_2$ in $(1)$. $\square$

\section*{2}
\subsection*{(1)}

Since $y_1,\dots ,y_n$ are random variables that satisfy:

\begin{equation*}
    \mathsf{P} ( y_i = 1 ) = 1 - \mathsf{P} ( y_i = 0 ) = \frac{1}{1 + \mathrm{e}^{- \langle x_i, \theta^\natural \rangle}} 
\end{equation*}

We knew that the probability of $\mathsf{P} ( y_i = 0 )$ is:

\begin{equation*}
    \begin{split}
        \mathsf{P} ( y_i = 0 ) 
        &= 1 - \mathsf{P} ( y_i = 1 ) \\
        &= \frac{1 + \mathrm{e}^{- \langle x_i, \theta^\natural \rangle}}{1 + \mathrm{e}^{- \langle x_i, \theta^\natural \rangle}} - \frac{1}{1 + \mathrm{e}^{- \langle x_i, \theta^\natural \rangle}} \\
        &= \frac{\mathrm{e}^{-\langle x_i, \theta^\natural \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta^\natural \rangle}} 
    \end{split}
\end{equation*}

We can write the pmf of given $x_i, \theta^{\natural}$, observed $y_i$ as:

\begin{equation*}
    p(y_i | x_i, \theta^{\natural}) = \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} \right)^{y_i} \left( \frac{\mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} \right)^{1 - 2y_i}
\end{equation*}

Using the pmf, we can get the likelihood function, which can be written as:

\begin{equation*}
    l(\theta) = \prod_{i=1}^n p(y_i | x_i, \theta)
    \footnote{Robert V. Hogg, Elliot A. Tanis, Dale Zimmerman, \textit{Probability and Statistical Inference}, 9th ed., Pearson Education, 2015, p. 258-259.}
\end{equation*}

We can further derive the $\log$-likelihood:

\begin{align*}
    \log l(\theta) 
    &= \log \left[\prod_{i=1}^n \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right)^{y_i} \left( \frac{\mathrm{e}^{-\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right)^{1 - 2y_i} \right] \\
    &= \sum_{i=1}^n \left[ y_i \log \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right) + (1 - 2y_i) \log \left( \frac{\mathrm{e}^{-\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right) \right] \tag{*}
\end{align*}

The terms in the above equation can be simplified:

\begin{equation*}
    \begin{split}
        y_i \log \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right) 
        &= y_i \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right)^{-1} \\
        &= -y_i \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right)
    \end{split}
\end{equation*}

\begin{equation*}
    \begin{split}
        (1 - 2y_i) \log \left( \frac{\mathrm{e}^{-\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right) 
        &= (1 - 2y_i) \log \left( \mathrm{e}^{-\langle x_i, \theta \rangle} \right) - (1 - 2y_i) \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right)\\
        &= -\langle x_i, \theta \rangle(1 - 2y_i) - \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) + y_i \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right)
    \end{split}
\end{equation*}

Plugging back into $(*)$, we get:

\begin{equation*}
    \begin{split}
        \log l(\theta) 
        &= \sum_{i=1}^n \left[ -y_i \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) - \langle x_i, \theta \rangle(1 - 2y_i) - \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) + y_i \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) \right] \\
        &= \sum_{i=1}^n \left[ - \langle x_i, \theta \rangle(1 - 2y_i) - \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) \right]
    \end{split}
\end{equation*}

We can find the maximum likelihood estimator $\hat{\theta}_n$ by maximizing the $\log$-likelihood function,
which is equivalent to find the minimum of the negative $\log$-likelihood function.

\begin{align*}
    - \log l(\theta) 
    &= - \sum_{i=1}^n \left[ - \langle x_i, \theta \rangle(1 - 2y_i) - \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) \right] \\
    &= \sum_{i=1}^n \left[ \langle x_i, \theta \rangle(1 - 2y_i) + \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) \right] \\
    &= \sum_{i=1}^n \left[ \log \mathrm{e}^{\langle x_i, \theta \rangle (1 - 2y_i)} + \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) \right] \\
    &= \sum_{i=1}^n \left[ \log \left( \mathrm{e}^{\langle x_i, \theta \rangle (1 - 2y_i)} \cdot \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) \right) \right] \\
    &= \sum_{i=1}^n \left[ \log \left( \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle} + \mathrm{e}^{(- 2y_i)} \right) \right] \\
\end{align*}

\begin{comment}
Thus, we should compute the gradient of the negative $\log$-likelihood w.r.t. $\theta$, set to $0$ and solve for $\theta$
\footnote{Deisenroth, Marc Peter, Faisal, A. Aldo, Ong, Cheng Soon, \textit{Mathematics for Machine Learning}, Cambridge University Press, 2020, pp. 351.}
:

\begin{equation*}
    \begin{split}
        \nabla \left( - \log l(\theta) \right) 
        &= \nabla \left( - \sum_{i=1}^n \left[ - \langle x_i, \theta \rangle(1 - 2y_i) - \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) \right] \right) \\
        &= \nabla \left( \sum_{i=1}^n \langle x_i, \theta \rangle(1 - 2y_i)\right) + \nabla \left(\sum_{i=1}^n\log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right)\right) \\
        &= \sum_{i=1}^n \left[ x_i(1 - 2y_i) - x_i \frac{\mathrm{e}^{-\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right] \\
        &= \sum_{i=1}^n \left[ \frac{x_i(1 - 2y_i)(1 + \mathrm{e}^{-\langle x_i, \theta \rangle})}{(1 + \mathrm{e}^{-\langle x_i, \theta \rangle})} - \frac{x_i \mathrm{e}^{-\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right] \\
    \end{split}
\end{equation*}
\end{comment}

\textcolor{red}{Check this part! (about the 1)}
\bigskip

Since $\mathrm{e}^{(- 2y_i)}$ evaluates to $1$ when $y_i = 0$ and is a small constant when $y_i = 1$,
also, taking the average (multiplying by $\frac{1}{n}$) will not change the result,
to minimize $- \log l(\theta)$ will be equivalent to minimize the given expression:

\begin{equation*}
\hat{\theta}_n \in \underset{\theta \in \mathbb{R}^p}{\operatorname{argmin}} L ( \theta ), \quad L (\theta) := \frac{1}{n} \sum_{i = 1}^n \log \left( 1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle} \right) \qquad \square
\end{equation*} 


\begin{comment}
Then $\hat{\theta}_n$ should also satisfy $\nabla L(\theta) = 0$, so we have:

In order to calculate it, we can use the fact that the term inside the summation is of the form:

\begin{equation*}
\log(1 + e^z)
\end{equation*}

So taking the derivative w.r.t. $z$ gives:

\begin{equation*}
\frac{d}{dz} \log(1 + e^z) = \frac{e^z}{1 + e^z}
\end{equation*}

Therefore we have $z = -2(y_i - \frac{1}{2})\langle x_i, \theta \rangle$, and combined using the chain rule will give:

\begin{align*}
\nabla L(\theta)
&= \nabla \left( \frac{1}{n} \sum_{i = 1}^n \log \left( 1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle} \right) \right) \\
&= \frac{1}{n} \sum_{i = 1}^n \nabla \left( \log \left( 1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle} \right) \right) \\
&= \frac{1}{n} \sum_{i = 1}^n \frac{1}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}} \cdot \nabla \left( 1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle} \right) \\
&= \frac{1}{n} \sum_{i = 1}^n \frac{\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}} \cdot \nabla \left( -2(y_i - \frac{1}{2})\langle x_i, \theta \rangle \right) \\
&= \frac{1}{n} \sum_{i = 1}^n \frac{\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}} \cdot -2(y_i - \frac{1}{2}) x_i = 0 \tag{1}
\end{align*}

Observe that when $y_i = 1$, the term in the summation will become:

\begin{equation*}
\frac{\mathrm{e}^{-\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \cdot - x_i
\end{equation*}

and if $y_i = 0$, the term will become:

\begin{equation*}
\frac{\mathrm{e}^{\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{\langle x_i, \theta \rangle}} \cdot x_i
\end{equation*}

Thus, we can rewrite $(1)$ as:

\begin{align*}
&n \cdot \mathsf{P}(y_i = 1) \cdot \left( \frac{- x_i \mathrm{e}^{-\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right) + n \cdot \mathsf{P}(y_i = 0) \cdot \left( \frac{x_i \mathrm{e}^{\langle x_i, \theta \rangle}}{1 + e^{\langle x_i, \theta \rangle}} \right) = 0 \\
\Rightarrow \ &n \cdot \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^\natural \rangle}} \cdot \left( \frac{- x_i}{1 + \mathrm{e}^{-\langle x_i, \theta^\natural \rangle}} \right) + n \cdot \left(\frac{\mathrm{e}^{-\langle x_i, \theta^\natural \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta^\natural \rangle}} \right) \cdot \left( \frac{x_i}{1 + \mathrm{e}^{\langle x_i, \theta^\natural \rangle}} \right) = 0
\end{align*}
\end{comment}

\subsection*{(2)}

To show that the optimization problem defining the maximum-likelihood estimator is convex,
we need to show that both the objective function and the constraint set are convex.
\bigskip

As in $1.(1)$, we knew that the constraint set $\mathbb{R}^p$ is convex,
therefore, we only need to check the convexity of the objective function.
\bigskip

Claim: The objective function 
$L (\theta) := \frac{1}{n} \sum_{i = 1}^n \log \left( 1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle} \right)$ is convex.

Following the same steps in $1.(1)$, we first differentiate $L (\theta)$ w.r.t. $\theta$:

\begin{align*}
    \nabla L(\theta) 
    &= \frac{1}{n} \sum_{i = 1}^n \frac{1}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}} \cdot -2(y_i - \frac{1}{2}) x_i \cdot \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle} \\
    &= \frac{1}{n} \sum_{i = 1}^n \frac{x_i(1 - 2y_i)\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}}
\end{align*}

To make the equation more readble, 
we can represent $z_i = 2(y_i - \frac{1}{2})\langle x_i, \theta \rangle$, 
so that $\nabla L(\theta)$ is equivalent to:

\begin{align*}
    \nabla L(\theta) 
    &= \frac{1}{n} \sum_{i = 1}^n \frac{x_i(1 - 2y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}}
\end{align*}

In order to calculate the Hessian, 
we first calculate some of the terms:

\begin{align*}
    \frac{d}{d\theta} z_i = 2(y_i - \frac{1}{2})x_i
\end{align*}

\begin{align*}
    \frac{d}{d\theta}e^{-z_i} = \textcolor{orange}{-2(y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i}}
\end{align*}

Then we'll have:

\begin{align*}
    \frac{d}{d\theta} \left( \frac{(1 - 2y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}} \right)
    &= \frac{\textcolor{orange}{\frac{d}{d\theta}}\left((1 - 2y_i)\mathrm{e}^{-z_i} \right) \cdot (1 + \mathrm{e}^{-z_i}) - (1 - 2y_i)\mathrm{e}^{-z_i} \cdot \textcolor{orange}{\frac{d}{d\theta}( 1 + \mathrm{e}^{-z_i})}}{(1 + \mathrm{e}^{-z_i})^2} \\
    &= \frac{\textcolor{orange}{-2}(1 - 2y_i)\textcolor{orange}{(y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i}} (1 + \mathrm{e}^{-z_i}) + \textcolor{orange}{2} (1 - 2y_i)\mathrm{e}^{-z_i} \textcolor{orange}{(y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i}}}{(1 + \mathrm{e}^{-z_i})^2} \\
    &= \frac{2(1 - 2y_i)(y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i}\left[(-1 - e^{-z_i}) + e^{-z_i}\right]}{(1 +\mathrm{e}^{-z_i})^2}\\
    &= 2(1 - 2y_i)(y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i}\frac{-1}{(1 +\mathrm{e}^{-z_i})^2} \\
    &= \frac{-2(1 - 2y_i)(y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2} 
\end{align*}

Getting back to the Hessian, we have:

\begin{align*}
    \textcolor{Green}{\nabla^2 L(\theta)}
    &= \frac{1}{n} \sum_{i = 1}^n \frac{d}{d\theta} \left[ x_i \left( \frac{(1 - 2y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}} \right) \right] \\
    &= \frac{1}{n} \sum_{i = 1}^n x_i \frac{-2(1 - 2y_i)(y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2} \\
    &\textcolor{Green}{= \frac{1}{n} \sum_{i = 1}^n x_i x_i^\intercal  \frac{-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2} \tag{1}}
\end{align*}

Since $(1 +\mathrm{e}^{-z_i})^2$ is strictly positive, the Hessian exists for all point in $\mathbb{R}^p$.
Therefore, $L(\theta)$ is twice differentiable.
\bigskip

Since we knew that the domain of $L(\theta)$ is convex, 
we only need to check if the Hessian is positive semidefinite to prove that $L(\theta)$ is convex.
\bigskip

By $(1)$, for any $v \in \mathbb{R}^p$, we have:

\begin{align*}
    v^\intercal  \nabla^2 L(\theta) v
    &= \frac{1}{n} \sum_{i = 1}^n v^\intercal  x_i x_i^\intercal  \frac{-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2} v \\
    &= \frac{1}{n} \sum_{i = 1}^n \frac{-2(1 - 2y_i)(y_i - \frac{1}{2})v^\intercal  x_i x_i^\intercal  v}{(1 +\mathrm{e}^{-z_i})^2} \\
\end{align*}

For the denominator, $(1 +\mathrm{e}^{-z_i})^2 > 0$, 
and for the coefficient, $-2(1 - 2y_i)(y_i - \frac{1}{2})$, 
since $y_i \in \{0, 1\}$, we have:

\begin{align*}
    -2(1 - 2y_i)(y_i - \frac{1}{2}) \ge 0
\end{align*}

Last, we have $v^\intercal  x_i x_i^\intercal  v$, this is equivelent to $(v^\intercal x_i)^2$, which is non-negative.

Therefore, we have:

\begin{align*}
    \frac{1}{n} \sum_{i = 1}^n \frac{-2(1 - 2y_i)(y_i - \frac{1}{2})v^\intercal  x_i x_i^\intercal  v}{(1 +\mathrm{e}^{-z_i})^2} \ge 0
\end{align*}

Thus the Hessian is positive semidefinite, and $L(\theta)$ is convex. $\square$

\subsection*{(3)}

Let: 
\begin{align*}
    X = \begin{bmatrix}
        x_1^\intercal  \\
        x_2^\intercal  \\
        \vdots \\
        x_n^\intercal 
    \end{bmatrix}
    \in \mathbb{R}^{n \times p}
    \qquad
    y = \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{bmatrix}
    \in \mathbb{R}^n
\end{align*}

By the previous subproblem, we have:

\begin{align*}
    \nabla L(\theta^{\natural}) 
    &= \frac{1}{n} \sum_{i = 1}^n \frac{x_i(1 - 2y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}} 
    \qquad \text{where } z_i = 2(y_i - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle \tag{*}
\end{align*}

Thus, to show that $\nabla L ( \theta^\natural ) = - \frac{1}{n} X^\intercal  (y - \mathsf{E} [ y ] )$,  
it is equivalent to prove:

\begin{align*}
    &\frac{1}{n} \sum_{i = 1}^n \frac{x_i(1 - 2y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}} = - \frac{1}{n} X^\intercal  (y - \mathsf{E} [ y ]) \\
    \Rightarrow \ &\sum_{i = 1}^n \frac{x_i(1 - 2y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}} = X^\intercal  (\mathsf{E} [ y ] - y) \tag{1}
\end{align*}

\begin{comment}
If we reformulate the left-hand side into matrix form, 
by using the definition of $X$, and define $u_i = \frac{(1 - 2y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}}$, 
we have:
    
\begin{align*}
    \sum_{i = 1}^n \frac{x_i(1 - 2y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}} 
    &= \sum_{i = 1}^n x_i u_i \\
    &= \begin{bmatrix}
        x_1 & x_2 & \cdots & x_n
    \end{bmatrix}
    \begin{bmatrix}
        u_1 \\
        u_2 \\
        \vdots \\
        u_n
    \end{bmatrix} \\
    &= X^\intercal u
\end{align*}

Therefore, the equation $(1)$ that we need to prove is equivalent to:

\begin{align*}
    X^\intercal u = X^\intercal  (\mathsf{E} [ y ] - y) \tag{2}
\end{align*}
\end{comment}

\begin{center}
    \textbf{Right-hand side of $(1)$}
\end{center}

First we knew that the definition of $\mathsf{E} [ y ]$ is:

\begin{align*}
    \mathsf{E} [ y ] 
    &= \begin{bmatrix}
        \mathsf{E} [ y_1 ] \\
        \mathsf{E} [ y_2 ] \\
        \vdots \\
        \mathsf{E} [ y_n ]
    \end{bmatrix} \\
\end{align*}

So:

\begin{align*}
    \mathsf{E} [ y ] - y 
    &= \begin{bmatrix}
        \mathsf{E} [ y_1 ] - y_1 \\
        \mathsf{E} [ y_2 ] - y_2 \\
        \vdots \\
        \mathsf{E} [ y_n ] - y_n
    \end{bmatrix}
\end{align*}

\begin{comment}
    Thus, to show $(2)$ is equivalent to showing:
    
    \begin{align*}
    \frac{(1 - 2y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}} = \mathsf{E} [ y_i ] - y_i \qquad \forall i \in [n] \quad \text{ where } z_i = 2(y_i - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle \tag{3}
    \end{align*}
\end{comment}

By subproblem $(1)$, we have:

\begin{align*}
    p(y_i | x_i, \theta^{\natural}) = \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} \right)^{y_i} \left( \frac{\mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} \right)^{1 - 2y_i}
\end{align*}

Therefore, its expected value is:

\begin{align*}
    \mathsf{E} [ y_i ] 
    &= 1 \times \mathsf{P} ( y_i = 1 ) + 0 \times \mathsf{P} ( y_i = 0 ) \\
    &= \mathsf{P} ( y_i = 1 ) \\
    &= \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}
\end{align*}


Therefore, we can rewrite the right-hand side of $(1)$:

\begin{align*}
    X^\intercal (\mathsf{E} [ y_i ] - y_i)
    &= \sum_{i = 1}^n \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} - y_i \right) x_i
\end{align*}

\textbf{Case.} For \( y_i = 1 \)
\bigskip

For the term in the summation, consider the case $y_i = 1$, this would evaluate to:

\begin{align*}
    (\mathsf{E} [ y_i ] - y_i ) x_i
    &= \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} - 1 \right) x_i \\
    &= \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} - \frac{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} \right) x_i \\
    &= \frac{- x_i \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} 
\end{align*}


\textbf{Case.} For \( y_i = 0 \)

\begin{align*}
    (\mathsf{E} [ y_i ] - y_i ) x_i
    &= \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} - 0 \right) x_i \\
    &= \frac{x_i}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}
\end{align*}

\bigskip

\begin{center}
    \textbf{Left-hand side of $(1)$}
\end{center}

Consider the left-hand side of $(1)$, expand the expression by plugging in the definition of $z_i$:

\begin{align*}
    \frac{x_i (1 - 2y_i)\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle}}
\end{align*}

\textbf{Case.} For \( y_i = 1 \)

\begin{align*}
    \frac{x_i (1 - 2)\mathrm{e}^{-2(1 - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-2(1 - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle}}
    = \frac{- x_i e^{-\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} 
\end{align*}

\textbf{Case.} For \( y_i = 0 \)

\begin{align*}
    \frac{x_i (1 - 0)\mathrm{e}^{-2(0 - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-2(0 - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle}}
    &= \frac{x_i \mathrm{e}^{\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{\langle x_i, \theta^{\natural} \rangle}}
\end{align*}

Combine all of the above into equation $(1)$, we found that the equation we need to prove :

\begin{align*}
    \sum_{i = 1}^n \frac{x_i (1 - 2y_i)\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle}} 
    = \sum_{i = 1}^n \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} - y_i \right) x_i \\
\end{align*}    

is actually the same:

\begin{align*}
    &\sum_{\{i | y_i = 1\}} \frac{- x_i \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} + \sum_{\{i | y_i = 0\}} \frac{x_i \mathrm{e}^{\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{\langle x_i, \theta^{\natural} \rangle}} 
    = \sum_{\{i | y_i = 1\}} \frac{- x_i \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} + \sum_{\{i | y_i = 0\}} \frac{x_i}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} \qquad \square
\end{align*}

\begin{comment}
--------------------------------

The lefthand side can be written as:

\begin{align*}
    \sum_{i = 1}^n \frac{x_i(1 - 2y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}}
    &= \sum_{i = 1}^n \frac{x_i\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}} - \sum_{i = 1}^n \frac{x_i y_i \mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}}
\end{align*}

So:

\begin{align*}
    -\frac{1}{n}X^\intercal  (y - \mathsf{E} [ y ])
    &= -\frac{1}{n}X^\intercal y + \frac{1}{n}X^\intercal \mathsf{E} [ y ] \\
    &= \frac{1}{n}
    \begin{bmatrix}
        x_1 & x_2 & \cdots & x_n
    \end{bmatrix}
    \begin{bmatrix}
        \mathsf{E} [ y_1 ]\\
        \mathsf{E} [ y_2 ] \\
        \vdots \\
        \mathsf{E} [ y_n ]
    \end{bmatrix}
    - \frac{1}{n}
    \begin{bmatrix}
        x_1 & x_2 & \cdots & x_n
    \end{bmatrix}
    \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{bmatrix} \\
    &= \frac{1}{n} \sum_{i = 1}^n (\mathsf{E} [ y_i ] - y_i )x_i  
\end{align*}





By the definition of $\nabla L(\theta^{\natural})$ in $(*)$, we have:

\begin{align*}
    \nabla L(\theta^{\natural}) 
    &= \frac{1}{n} \sum_{i = 1}^n \frac{x_i(1 - 2y_i)\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle}} \\
\end{align*}

=== could be wrong ===
\bigskip

And since $\theta^{\natural}$ is the true parameter, 
this implies that it would minimize the error function $L(\theta)$, 
which is equivalent to satisfy:

\begin{align*}
    \nabla L(\theta^{\natural}) = 0
\end{align*}
\end{comment}

\subsection*{(4)}

By equation $(1)$ in $2. (2)$, we have:

\begin{align*}
    \nabla^2 L(\theta^{\natural}) = \frac{1}{n} \sum_{i = 1}^n x_i x_i^\intercal  \frac{-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2}
    \qquad \text{where } z_i = 2(y_i - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle 
\end{align*}

As the approach used in the previous subproblem, we can rewrite the summation part 
by deviding into the cases that $y_i = 1$ and $y_i = 0$:

\begin{align*}
    \nabla^2 L(\theta^{\natural}) 
    &= \frac{1}{n} \left[ \sum_{\{i | y_i = 1\}} x_i x_i^\intercal  \frac{-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2} +\sum_{\{i | y_i = 0\}} x_i x_i^\intercal  \frac{-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2} \right] \\
    &= \frac{1}{n} \left[ \sum_{\{i | y_i = 1\}} x_i x_i^\intercal  \frac{-2(1 - 2)(1 - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2} +\sum_{\{i | y_i = 0\}} x_i x_i^\intercal  \frac{-2(1 - 0)(0 - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2} \right] \\
    &= \frac{1}{n} \left[ \sum_{\{i | y_i = 1\}} x_i x_i^\intercal  \frac{\mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{(1 +\mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle})^2} +\sum_{\{i | y_i = 0\}} x_i x_i^\intercal  \frac{\mathrm{e}^{\langle x_i, \theta^{\natural} \rangle}}{(1 +\mathrm{e}^{\langle x_i, \theta^{\natural} \rangle})^2} \right] \tag{1} \\
\end{align*}

Need to prove that:

\begin{align*}
    \nabla^2 L(\theta^{\natural})  = X^\intercal D X 
    \qquad \text{where } D = \begin{bmatrix}
        Var(y_1) & 0 & \cdots & 0 \\
        0 & Var(y_2) & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & Var(y_n)
    \end{bmatrix}
\end{align*}

To calculate $Var(y_i)$, we knew that:

\begin{align*}
    Var(y_i) = \mathsf{E} [ y_i^2 ] - \mathsf{E} [ y_i ]^2
\end{align*}

Since $y_i \in \{0, 1\}$, and from the previous subproblem $2. (3)$, we have:

\begin{align*}
    \mathsf{E} [ y_i ] = \mathsf{P} ( y_i = 1 ) = \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}
\end{align*}

So we can derive the following:

\begin{align*}
    \mathsf{E} [ y_i^2 ] 
    &= \mathsf{P} ( y_i = 1 ) \cdot 1^2 + \mathsf{P} ( y_i = 0 ) \cdot 0^2 \\
    &= \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}
\end{align*}


\begin{align*}
    \mathsf{E} [ y_i ]^2 = \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} \right)^2
\end{align*}

Hence, we have:

\begin{align*}
    Var(y_i) = \mathsf{E} [ y_i^2 ] - \mathsf{E} [ y_i ]^2
    &= \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} - \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} \right)^2 \\
    &= \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} \left( 1 - \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} \right) \\
    &= \frac{\mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{(1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle})^2}
\end{align*}

Calculate $\frac{1}{n}X^\intercal D X$:

\begin{align*}
    \frac{1}{n}X^\intercal D X
    &= \frac{1}{n} \begin{bmatrix}
        x_1 & x_2 & \cdots & x_n
    \end{bmatrix}
    \begin{bmatrix}
        \frac{\mathrm{e}^{-\langle x_1, \theta^{\natural} \rangle}}{(1 + \mathrm{e}^{-\langle x_1, \theta^{\natural} \rangle})^2} & 0 & \cdots & 0 \\
        0 & \frac{\mathrm{e}^{-\langle x_2, \theta^{\natural} \rangle}}{(1 + \mathrm{e}^{-\langle x_2, \theta^{\natural} \rangle})^2} & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \frac{\mathrm{e}^{-\langle x_n, \theta^{\natural} \rangle}}{(1 + \mathrm{e}^{-\langle x_n, \theta^{\natural} \rangle})^2}
    \end{bmatrix}
    \begin{bmatrix}
        x_1^\intercal \\ x_2^\intercal \\ \vdots \\ x_n^\intercal
    \end{bmatrix} \\
    &= \frac{1}{n} \sum_{i = 1}^n \frac{x_i x_i^\intercal \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{(1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle})^2}
\end{align*}

This result can also be split into the cases that $y_i = 1$ and $y_i = 0$:

\begin{align*}
    \frac{1}{n} \sum_{i = 1}^n \frac{x_i x_i^\intercal \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{(1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle})^2}
    &= \frac{1}{n} \left[ \sum_{\{i | y_i = 1\}} \frac{x_i x_i^\intercal \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{(1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle})^2} +\sum_{\{i | y_i = 0\}} \frac{x_i x_i^\intercal \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle} \cdot (\mathrm{e}^{\langle x_i, \theta^{\natural} \rangle})^2}{(1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle})^2 \cdot (\mathrm{e}^{\langle x_i, \theta^{\natural} \rangle})^2} \right]\\
    &= \frac{1}{n} \left[ \sum_{\{i | y_i = 1\}} \frac{x_i x_i^\intercal \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{(1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle})^2} +\sum_{\{i | y_i = 0\}} \frac{x_i x_i^\intercal \mathrm{e}^{\langle x_i, \theta^{\natural} \rangle}}{(1 + \mathrm{e}^{\langle x_i, \theta^{\natural} \rangle})^2} \right]
\end{align*}

Which is exactly the same as equation $(1)$, therefore, we have proven that $\nabla^2 L ( \theta^\natural ) = \frac{1}{n} X^{\mathsf{\scriptscriptstyle{T}}} D X$ holds. $\square$

\subsection*{(5)}

In the last part of the subproblem $2. (2)$, we have already shown that $0 \le \nabla^2 L (\theta)$.

Therefore, we need to show that:

\begin{align*}
    \nabla^2 L (\theta) \leq \frac{\lambda_{\max}( X^{\mathsf{\scriptscriptstyle{T}}} X )}{4 n } I, \quad \forall \theta \in \mathbb{R}^p
\end{align*}

Which means that we need to show:

\begin{align*}
    \frac{\lambda_{\max}( X^{\mathsf{\scriptscriptstyle{T}}} X )}{4 n } I - \nabla^2 L (\theta), \quad \forall \theta \in \mathbb{R}^p
\end{align*}

is positive semi-definite.
\bigskip

Since the expression $\in \mathbb{R}^{p \times p}$, 
given arbitrary $u \in \mathbb{R}^p$, we need to show that:

\begin{align*}
    &u^\intercal \left( \frac{\lambda_{\max}( X^{\mathsf{\scriptscriptstyle{T}}} X )}{4 n } I - \nabla^2 L (\theta) \right) u \geq 0 \\
    \Rightarrow \ &u^\intercal \frac{\lambda_{\max}( X^{\mathsf{\scriptscriptstyle{T}}} X )}{4 n } I u \geq u^\intercal \nabla^2 L (\theta) u \\
\end{align*}

Plugging in the expression of $\nabla^2 L (\theta)$ from equation $(1)$ in subproblem $2. (2)$, we have:

\begin{align*}
    u^\intercal \textcolor{Green}{\nabla^2 L (\theta)} u 
    &= u^\intercal \textcolor{Green}{\frac{1}{n} \sum_{i = 1}^n x_i x_i^\intercal  \frac{-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2}} u \\
    &= \frac{1}{n} \sum_{i = 1}^n u^\intercal x_i x_i^\intercal u \frac{(-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-z_i})}{(1 +\mathrm{e}^{-z_i})^2} \\
    &= \frac{1}{n} \sum_{i = 1}^n u^\intercal x_i x_i^\intercal u \textcolor{Thistle}{\frac{-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2}} \qquad \text{where } z_i = 2(y_i - \frac{1}{2})\langle x_i, \theta \rangle \\
\end{align*}

Consider the possible values of the term 
$\textcolor{Thistle}{\frac{-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2}}$:
\bigskip

\textbf{Case.} For \( y_i = 1 \)

\begin{align*}
    \frac{-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}}{(1 +\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle})^2}
    &= \frac{-2(1 - 2)(1 - \frac{1}{2})\mathrm{e}^{-2(1 - \frac{1}{2})\langle x_i, \theta \rangle}}{(1 +\mathrm{e}^{-2(1 - \frac{1}{2})\langle x_i, \theta \rangle})^2} \\
    &= \frac{\mathrm{e}^{-\langle x_i, \theta \rangle}}{(1 +\mathrm{e}^{-\langle x_i, \theta \rangle})^2}
\end{align*}

Since we need to check if the maximum possible value of $\nabla^2 L (\theta)$ would exceed $\frac{\lambda_{\max}( X^{\mathsf{\scriptscriptstyle{T}}} X )}{4 n } I$,
we need to find the maximum possible value of this term.
\bigskip

First, since $\mathrm{e}^{-z_i}$ is always positive, $\frac{\mathrm{e}^{-\langle x_i, \theta \rangle}}{(1 +\mathrm{e}^{-\langle x_i, \theta \rangle})^2} \ge 0$.

Then, let:

\begin{align*}
    f(x) = \frac{\mathrm{e}^{-x}}{(1 +\mathrm{e}^{-x})^2}
\end{align*}

Differentiate:

\begin{align*}
    f'(x) 
    &= \frac{(-\mathrm{e}^{-x})(1 +\mathrm{e}^{-x})\textcolor{Periwinkle}{^2} - \mathrm{e}^{-x} \cdot 2\textcolor{Periwinkle}{(1 +\mathrm{e}^{-x})}(-\mathrm{e}^{-x})}{(1 +\mathrm{e}^{-x})\textcolor{Periwinkle}{^4}} \\
    &= \frac{\textcolor{Plum}{(-\mathrm{e}^{-x})}(1 +\mathrm{e}^{-x}) - 2\mathrm{e}^{-x} \textcolor{Plum}{(-\mathrm{e}^{-x})}}{(1 +\mathrm{e}^{-x})^3} \\
    &= \frac{(-\mathrm{e}^{-x})(1 - \mathrm{e}^{-x})}{(1 +\mathrm{e}^{-x})^3} \\
\end{align*}

Set $f'(x) = 0$:

\begin{align*}
    \frac{(-\mathrm{e}^{-x})(1 - \mathrm{e}^{-x})}{(1 +\mathrm{e}^{-x})^3} = 0
\end{align*}

Since $-\mathrm{e}^{-x} \ne 0$, we must have $1 - \mathrm{e}^{-x} = 0$, which means $\mathrm{e}^{-x} = 1$.
Therefore, $f(x)$ has critical point at $x = 0$.

Calculate $f(0)$:

\begin{align*}
    f(0) = \frac{\mathrm{e}^{0}}{(1 +\mathrm{e}^{0})^2} = \frac{1}{4}
\end{align*}

Check the second derivative:
\bigskip

\begin{align*}
    f''(x) 
    &= \frac{\left[(-\mathrm{e}^{-x})(1 - \mathrm{e}^{-x}) \right]'(1 +\mathrm{e}^{-x})^3 - \left[ (1 +\mathrm{e}^{-x})^3 \right]'\left[(-\mathrm{e}^{-x})(1 - \mathrm{e}^{-x}) \right]}{(1 +\mathrm{e}^{-x})^6} \\
    &= \frac{\textcolor{Plum}{-\mathrm{e}^{-x}} (1 - 2\mathrm{e}^{-x})(1 +\mathrm{e}^{-x})\textcolor{Plum}{^3} - 3\textcolor{Plum}{(1 +\mathrm{e}^{-x})^2 \mathrm{e}^{-x}} (-1) (-\mathrm{e}^{-x})(1 - \mathrm{e}^{-x})}{(1 +\mathrm{e}^{-x})^6} \\
    &= \frac{\textcolor{Plum}{\mathrm{e}^{-x} (1 +\mathrm{e}^{-x})^2} \left[ (1 - 2\mathrm{e}^{-x})(1 +\mathrm{e}^{-x}) + 3 (-\mathrm{e}^{-x}) (1 - \mathrm{e}^{-x}) \right]}{(1 +\mathrm{e}^{-x})^6} \\
    &= \frac{\mathrm{e}^{-x} (1 +\mathrm{e}^{-x})^2 \left[ 1 - 2\mathrm{e}^{-x} + \mathrm{e}^{-x} - 2 \mathrm{e}^{-2x} - 3 \mathrm{e}^{-x} + 3 \mathrm{e}^{-2x} \right]}{(1 +\mathrm{e}^{-x})^6} \\
    &= \frac{\mathrm{e}^{-x} (1 +\mathrm{e}^{-x})^2 \left[ 1 - 4\mathrm{e}^{-x} + \mathrm{e}^{-2x} \right]}{(1 +\mathrm{e}^{-x})^6} \\
    &= \frac{\mathrm{e}^{-x} \left[ 1 - 4\mathrm{e}^{-x} + \mathrm{e}^{-2x} \right]}{(1 +\mathrm{e}^{-x})^4}
\end{align*}

Evaluate at $x = 0$:

\begin{align*}
    f''(0) = \frac{\mathrm{e}^0 (1 - 4\mathrm{e}^0 + \mathrm{e}^{0})}{(1 + \mathrm{e}^0)^4} = \frac{1 - 4 + 1}{16} = -\frac{1}{8}
\end{align*}

Since $f''(0) < 0$, $f(x)$ has a local maximum at $x = 0$.
\bigskip

\textbf{Case.} For \( y_i = 0 \)

\begin{align*}
    \frac{-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}}{(1 +\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle})^2}
    &= \frac{-2(1 - 0)(0 - \frac{1}{2})\mathrm{e}^{-2(0 - \frac{1}{2})\langle x_i, \theta \rangle}}{(1 +\mathrm{e}^{-2(0 - \frac{1}{2})\langle x_i, \theta \rangle})^2} \\
    &= \frac{\mathrm{e}^{\langle x_i, \theta \rangle}}{(1 +\mathrm{e}^{\langle x_i, \theta \rangle})^2}
\end{align*}

Checking the maximum possible value of this term again, let:

\begin{align*}
    g(x) = \frac{\mathrm{e}^{x}}{(1 +\mathrm{e}^{x})^2}
\end{align*}

Differentiate:

\begin{align*}
    g'(x) 
    &= \frac{\mathrm{e}^{x}(1 +\mathrm{e}^{x})\textcolor{Periwinkle}{^2} - \mathrm{e}^{x} \cdot 2\textcolor{Periwinkle}{(1 +\mathrm{e}^{x})}\mathrm{e}^{x}}{(1 +\mathrm{e}^{x})\textcolor{Periwinkle}{^4}} \\
    &= \frac{\textcolor{Plum}{\mathrm{e}^{x}}(1 +\mathrm{e}^{x}) - 2\mathrm{e}^{x} \cdot \textcolor{Plum}{\mathrm{e}^{x}}}{(1 +\mathrm{e}^{x})^3} \\
    &= \frac{\mathrm{e}^{x} (1 - \mathrm{e}^{x})}{(1 +\mathrm{e}^{x})^3} \\
\end{align*}

Set $g'(x) = 0$:

\begin{align*}
    \frac{\mathrm{e}^{x} (1 - \mathrm{e}^{x})}{(1 +\mathrm{e}^{x})^3} = 0
\end{align*}

Since $\mathrm{e}^{x} \ne 0$, we must have $1 - \mathrm{e}^{x} = 0$, which means $\mathrm{e}^{x} = 1$.
Therefore, $g(x)$ has critical point at $x = 0$.
\bigskip

Calculate $g(0)$:

\begin{align*}
    g(0) = \frac{\mathrm{e}^{0}}{(1 +\mathrm{e}^{0})^2} = \frac{1}{4}
\end{align*}

Check the second derivative:

\begin{align*}
    g''(x)
    &= \frac{\left[\mathrm{e}^x(1 - \mathrm{e}^x)\right]'(1 + \mathrm{e}^x)^3 - \left[ (1 + \mathrm{e}^x)^3 \right]'\mathrm{e}^x (1 - \mathrm{e}^x)}{(1 + \mathrm{e}^x)^6} \\
    &= \frac{\textcolor{Plum}{\mathrm{e}^x}(1 - 2 \mathrm{e}^x)(1 + \mathrm{e}^x)\textcolor{Plum}{^3} - 3 \textcolor{Plum}{\mathrm{e}^x (1 + \mathrm{e}^x)^2}\mathrm{e}^x (1 - \mathrm{e}^x)}{(1 + \mathrm{e}^x)^6} \\
    &= \frac{\textcolor{Plum}{\mathrm{e}^x(1 + \mathrm{e}^x)^2} \left[ (1 - 2 \mathrm{e}^x)(1 + \mathrm{e}^x) - 3 \mathrm{e}^x (1 - \mathrm{e}^x) \right]}{(1 + \mathrm{e}^x)^6} \\
    &= \frac{\mathrm{e}^x(1 + \mathrm{e}^x)^2 \left[ 1 - 2 \mathrm{e}^x + \mathrm{e}^x - 2 \mathrm{e}^{2x} - 3 \mathrm{e}^x + 3 \mathrm{e}^{2x} \right]}{(1 + \mathrm{e}^x)^6} \\
    &= \frac{\mathrm{e}^x(1 + \mathrm{e}^x)^2 \left[ 1 - 4\mathrm{e}^x + \mathrm{e}^{2x} \right]}{(1 + \mathrm{e}^x)^6} \\
    &= \frac{\mathrm{e}^x \left[ 1 - 4\mathrm{e}^x + \mathrm{e}^{2x} \right]}{(1 + \mathrm{e}^x)^4} \\
\end{align*}

Evaluate at $x = 0$:

\begin{align*}
    g''(0) &= \frac{\mathrm{e}^0 (1 - 4\mathrm{e}^0 + \mathrm{e}^{0})}{(1 + \mathrm{e}^0)^4} \\
    &= \frac{1 - 4 + 1}{16} \\
    &= -\frac{1}{8}
\end{align*}

Since $g''(0) < 0$, $g(x)$ has a local maximum at $x = 0$.
\bigskip

Thus, by the above two cases, 
we found that the maximum possible value of 
$\frac{-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}}{(1 +\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle})^2}$ 
is bounded by$\frac{1}{4}$, hence we have:

\begin{align*}
    u^\intercal \nabla^2 L (\theta) u 
    &= \frac{1}{n} \sum_{i = 1}^n u^\intercal x_i x_i^\intercal u \textcolor{Thistle}{\frac{-2(1 - 2y_i)(y_i - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2}} \\
    &\textcolor{Thistle}{\leq} \frac{1}{n} \sum_{i = 1}^n u^\intercal x_i x_i^\intercal u \textcolor{Thistle}{\frac{1}{4}} \\
    &= \frac{1}{4n} u^\intercal \sum_{i=1}^n (x_i x_i^\intercal) u \\
\end{align*}

Which we could observe that since:

\begin{align*}
    X^\intercal X 
    = \begin{bmatrix}
        x_1 & x_2 & \cdots & x_n
    \end{bmatrix}
    \begin{bmatrix}
        x_1^\intercal \\ x_2^\intercal \\ \vdots \\ x_n^\intercal
    \end{bmatrix}
    = \sum_{i=1}^n x_i x_i^\intercal
\end{align*}

We have:

\begin{align*}
    \textcolor{Green}{u^\intercal \nabla^2 L (\theta) u} 
    &\leq \textcolor{BlueGreen}{\frac{1}{4n} u^\intercal X^\intercal X u} \\
\end{align*}

By definition, since $X^\intercal X \in \mathbb{R}^{p \times p}$ is a symmetric matrix,
let $Q(u) = u^\intercal X^\intercal X u$ is its quadratic form.
\footnote{G. Chen, "Lecture 4: Rayleigh Quotient," San Jose State University, p.4. Available at: \url{https://www.sjsu.edu/faculty/guangliang.chen/Math253S20/lec4RayleighQuotient.pdf}.}
\bigskip

Then, its Rayleigh quotient is to normalize $Q(u)$ by $u^\intercal u$, 
and its maximum value is the maximum eigenvalue of $X^\intercal X$ (i.e. $\lambda_{\max}(X^\intercal X)$).
\footnote{G. Chen, "Lecture 4: Rayleigh Quotient," San Jose State University, p.10-11. Available at: \url{https://www.sjsu.edu/faculty/guangliang.chen/Math253S20/lec4RayleighQuotient.pdf}.}
:

\begin{align*}
    &\max_{u \in \mathbb{R}^p} \frac{u^\intercal X^\intercal X u}{u^\intercal u}
    = \lambda_{\max}(X^\intercal X) \\
    \Rightarrow \ & u^\intercal X^\intercal X u
    \leq \lambda_{\max}(X^\intercal X) u^\intercal u \\
    \Rightarrow \ & \textcolor{BlueGreen}{\frac{1}{4n} u^\intercal X^\intercal X u}
    \leq \textcolor{orange}{\frac{1}{4n} \lambda_{\max}(X^\intercal X) u^\intercal u} \\
\end{align*}

Therefore, we have:

\begin{align*}
    &\textcolor{Green}{u^\intercal \nabla^2 L (\theta) u} 
    \leq \textcolor{orange}{\frac{1}{4n} \lambda_{\max}(X^\intercal X) u^\intercal u} \\
    \Rightarrow \ & \nabla^2 L (\theta) \leq \frac{\lambda_{\max}(X^\intercal X)}{4n} I \quad \square
\end{align*}

\section*{3}

\subsection*{(1)}

The optimization problem is defined as follows:

\begin{align*}
    x_\star \in \underset{x \in \Delta_d} {\operatorname{argmin}} f ( x ), \quad f ( x ) := - \sum_{i = 1}^n w_i \log \langle a_i, x \rangle , \tag{P}
\end{align*}

To show that $P$ is convex, same theorem and approach used in problem $1. (1)$ will be used again.
\bigskip

First, we need to show that the objective function $f$ is convex, and the constraint set $\Delta_d$ is convex.

\begin{center}
    \textbf{Prove that constraint set $\Delta_d$ is convex}
\end{center}

To prove this by definition, 
we need to show that for any $x_1, x_2 \in \Delta_d$ and $\lambda \in [0, 1]$,

\begin{align*}
    x' = \lambda x_1 + (1 - \lambda) x_2 \in \Delta_d
\end{align*}

By the definition of $\Delta_d$:

\begin{align*}
    \Delta_d := \left\{ x = ( x[1], \ldots, x[d] ) \in \mathbb{R}^d \, \middle| \, x[i] \geq 0, \sum_{i = 1}^d x[i] = 1 \right\} 
\end{align*}

this means that $x'$ should satisfy:

\begin{align*}
    x'[i] &\geq 0, \quad \forall i \in \{1, \ldots, d\} \\
    \sum_{i = 1}^d x'[i] &= 1
\end{align*}

Since $x_1, x_2 \in \Delta_d$, we knew that:

\begin{align*}
x[i] \geq 0, \quad y[i] \geq 0 \quad \forall i \in \{1, \dots, d\}
\end{align*}

Also, $\lambda \in [0, 1]$, so both $\lambda$ and $1 - \lambda$ are nonnegative, 
so for arbitrary $i \in \{1, \ldots, d\}$, we have:

\begin{align*}
    x'[i] = \lambda x_1[i] + (1 - \lambda) x_2[i] \geq 0
\end{align*}

Next, we need to check the sum:

\begin{align*}
    \sum_{i = 1}^d x'[i] 
    &= \sum_{i = 1}^d \lambda x_1[i] + (1 - \lambda) x_2[i] \\
    &= \lambda \sum_{i = 1}^d x_1[i] + (1 - \lambda) \sum_{i = 1}^d x_2[i] \\
    &= \lambda \cdot 1 + (1 - \lambda) \cdot 1 \\
    &= 1
\end{align*}

Therefore, $x' \in \Delta_d$, and $\Delta_d$ is convex.
\bigskip

\begin{center}
    \textbf{Prove that objective function $f$ is convex}
\end{center}

To use the theorem in $1. (1)$ , we need to first show that $f$ is twice differentiable,
then prove that its Hessian matrix is positive semidefinite.
\bigskip

First, we show that $f$ is twice differentiable.

\begin{align*}
    &f(x) = - \sum_{i = 1}^n w_i \log \langle a_i, x \rangle \\
    \Rightarrow \ & \nabla f(x) = -\sum_{i=1}^{n} w_i \frac{a_i}{\langle a_i, x \rangle} \\
\end{align*}

Since $\langle a_i, x \rangle$ is linear in $x$, 
and $a_i$ is an entry-wise nonnegative vector $\ni a_i \ne 0$,
thus $\langle a_i, x \rangle > 0$.
Therefore, for each point $x$ in the domain of $f$, $\nabla f(x)$ exists,
and $f$ is differentiable.
\bigskip

Then we check the second derivative:

\begin{align*}
    \nabla^2 f(x) 
    &= - \sum_{i=1}^{n} w_i \nabla \left( \frac{a_i}{\langle a_i, x \rangle} \right) \\
    &= - \sum_{i=1}^{n} w_i \frac{\nabla (a_i) \langle a_i, x \rangle - a_i \nabla (\langle a_i, x \rangle)}{\langle a_i, x \rangle^2} \\
    &= - \sum_{i=1}^{n} w_i \frac{0 - a_i a_i^\intercal}{\langle a_i, x \rangle^2} \\
    &= \sum_{i=1}^{n} w_i \frac{a_i a_i^\intercal}{\langle a_i, x \rangle^2} \\
\end{align*}

It is trivial that $\nabla^2 f(x)$ exists,
so we can then check if it is positive semidefinite.

For any $x \in \Delta_d$, we have:

\begin{align*}
    x^\intercal \nabla^2 f(x) x 
    &= x^\intercal \sum_{i=1}^{n} w_i \frac{a_i a_i^\intercal}{\langle a_i, x \rangle^2} x \\
    &= \sum_{i=1}^{n} w_i \frac{x^\intercal a_i a_i^\intercal x}{\langle a_i, x \rangle^2} \\
    &= \sum_{i=1}^{n} w_i \frac{(a_i^\intercal x)^2}{\langle a_i, x \rangle^2} \ge 0 \\
\end{align*}

We have the above expression $\ge 0$ since $w_i$ are given nonnegative.
Therefore, $\nabla^2 f(x)$ is positive semidefinite,
and $f$ is convex. $\square$

\subsection*{(2)}

As stated in the textbook, 
the ellipsoid method computes $g(x^{(k)})$ via formula $(3.2)$ in the previous cutting plane method.
\footnote{Y.-T. Wong, *Techniques in optimization and sampling* (Draft),p.30. Available at: \url{https://github.com/YinTat/optimizationbook}.}
Which is given by:

\begin{align*}
    H^{(k)} = \{x \in \mathbb{R}^n \ | \ g(x^{(k)})^\intercal (x - x^{(k)}) \le 0\}
\end{align*}

Also, in the remark of Problem $3. (1)$
\footnote{Y.-T. Wong, *Techniques in optimization and sampling* (Draft),p.29. Available at: \url{https://github.com/YinTat/optimizationbook}.}, 
it said that we set $g(x) = \nabla f(x)$, 
and the gurantee for $g(\cdot)$ implies that $K$ lies in the half space:

\begin{align*}
    H^{(k)} = \{y \ | \ g(x^{(k)})^\intercal (y - x^{(k)}) \le 0\}
\end{align*}

Which means that the mapping $g(\cdot)$ is chosen by calculating the gradient of $f(x)$.
And this has already been done in the previous subproblem $3. (1)$.
\bigskip

Therefore, we have:

\begin{align*}
    g(x) = \nabla f(x) = -\sum_{i=1}^{n} w_i \frac{a_i}{\langle a_i, x \rangle} \quad \square
\end{align*}

\subsection*{(3)}

From the original definition of the ellipsoid, we have:

\begin{align*}
    E^{(k)} = \{ y \in \mathbb{R}^n \ | \ (y - x^{(k)})^\intercal (A^{(k)})^{-1} (y - x^{(k)}) \le 1 \} \tag{1}
\end{align*}

in the proof of Lemma $3.3$, 
it said that the affine transformation would transform it into the following by setting
$A^{(k)} = I, \quad x^{(k)} = 0, \quad v ( x^{(k)} ) = e_1$:

\begin{align*}
    E^{(k)} = \{ y \in \mathbb{R}^n \ | \ y^\intercal y \le 1 \} \tag{2}
\end{align*}

And would therfore make the halfspace be transformed from its orignal definition (which we mentioned in the previous subproblem $3. (2)$):

\begin{align*}
    H^{(k)} = \{x \in \mathbb{R}^n \ | \ g(x^{(k)})^\intercal (x - x^{(k)}) \le 0\}
\end{align*}

to:

\begin{align*}
    H^{(k)} = \{ x \ | \ x_1 \le 0 \}
\end{align*}

So, since affine transformation would not change set containment,
suppose we have $y_1 \in E^{(k)}$, then it will satisfy:

\begin{align*}
    (y_1 - x^{(k)})^\intercal (A^{(k)})^{-1} (y_1 - x^{(k)}) \le 1 \tag{*}
\end{align*}

based on the definition in $(1)$, and after some affine transformation:

\begin{align*}
    y_1' = Ay_1 + b \in E^{(k)} \tag{3}
\end{align*}

and $y_1'$ satisfies:

\begin{align*}
    (y_1')^\intercal y_1' \le 1 \tag{4}
\end{align*}

based on the definition in $(2)$.
\bigskip

Thus we can derive the following by plugging $(3)$ into $(4)$:

\begin{align*}
    &(Ay_1 + b )^\intercal (Ay_1 + b ) \le 1 \\
    \Rightarrow \ & (y_1^\intercal A^\intercal + b^\intercal)(Ay_1 + b) \le 1 \\
    \Rightarrow \ & \textcolor{Green}{y_1^\intercal A^\intercal Ay_1 + y_1^\intercal A^\intercal b + b^\intercal A y_1 + b^\intercal b \le 1} \\
\end{align*}

Expand $(*)$:

\begin{align*}
    &(y_1 - x^{(k)})^\intercal (A^{(k)})^{-1} (y_1 - x^{(k)}) \le 1 \\
    \Rightarrow \ & (y_1^\intercal - (x^{(k)})^\intercal) (A^{(k)})^{-1} (y_1 - x^{(k)}) \le 1 \\
    \Rightarrow \ & \left[y_1^\intercal(A^{(k)})^{-1} - (x^{(k)})^\intercal(A^{(k)})^{-1}\right] (y_1 - x^{(k)}) \le 1 \\
    \Rightarrow \ & \textcolor{orange}{y_1^\intercal(A^{(k)})^{-1} y_1 - y_1^\intercal(A^{(k)})^{-1} x^{(k)} - x^{(k)\intercal}(A^{(k)})^{-1} y_1 + x^{(k)\intercal}(A^{(k)})^{-1} x^{(k)} \le 1} \\
\end{align*}

Compare the two expressions, and we can observe that:

\begin{align*}
    &\textcolor{Green}{y_1^\intercal A^\intercal Ay_1} = \textcolor{orange}{y_1^\intercal(A^{(k)})^{-1} y_1} \\
    \Rightarrow \ & A^\intercal A = (A^{(k)})^{-1} \\
    \Rightarrow \ & \textcolor{orange}{- y_1^\intercal(A^{(k)})^{-1} x^{(k)}} = - y_1^\intercal A^\intercal A x^{(k)} = \textcolor{Green}{y_1^\intercal A^\intercal b} \\
    \Rightarrow \ & b = -A x^{(k)}
\end{align*}

Verify the result:

\begin{align*}
    &\textcolor{orange}{- x^{(k)\intercal}(A^{(k)})^{-1} y_1} = - x^{(k)\intercal} A^\intercal A y_1 = \textcolor{Green}{b^\intercal A y_1} \\
    &\textcolor{orange}{x^{(k)\intercal}(A^{(k)})^{-1} x^{(k)}} = (-A x^{(k)})^\intercal (-A x^{(k)}) = \textcolor{Green}{b^\intercal b}
\end{align*}

Therefore, we have:

\begin{align*}
    \begin{cases}
        A^\intercal A = (A^{(k)})^{-1} \\
        b = -A x^{(k)}
    \end{cases}
\end{align*}

To further derive $A$, we knew that $A$ is invertible,
and $A^\intercal A = (A^\intercal A)^\intercal$, so $A^\intercal A$ is Hermitian.
Also, consider any $u \in \mathbb{R}^n, \quad u \ne 0$, we have:

\begin{align*}
    u^\intercal (A^\intercal A) u = (A u)^\intercal (A u)  = \|A u\|^2 > 0
\end{align*}

Note that we have $>$ since $A$ is invertible and $u \ne 0$.
Thus, $A^\intercal A$ is positive definite.
\bigskip

Using the above result ($A^\intercal A$: positive definite, Hermitian),
we knew that $A^\intercal A$ can be written as a product of its square root matrix.
\footnote{Wikipedia contributors, "Cholesky decomposition," \textit{Wikipedia, The Free Encyclopedia}, Available at: \url{https://en.wikipedia.org/wiki/Cholesky_decomposition}. Accessed: Mar. 10, 2025.}

\begin{align*}
    (A^\intercal A)^{\frac{1}{2}} = ((A^{(k)})^{-1})^{\frac{1}{2}}
\end{align*}

Therefore, we have:

\begin{align*}
    A = ((A^{(k)})^{-1})^{\frac{1}{2}}
\end{align*}
\footnote{Wikipedia contributors, "Square root of a matrix," \textit{Wikipedia, The Free Encyclopedia}, Available at: \url{https://en.wikipedia.org/wiki/Square_root_of_a_matrix}. Accessed: Mar. 10, 2025.}

And we conclude that the affine transformation is given by:

\begin{align*}
    A y_1 + b \quad \text{ where }
    A = ((A^{(k)})^{-1})^{\frac{1}{2}}, \quad b = -A x^{(k)} \quad \square
\end{align*}



\begin{comment}
\begin{align*}
    y_1 = A^{-1}(y_1' - b) 
\end{align*}

since $A$ is guaranteed to be invertible by the definition of affine transformation.
Plugging it into $(*)$, we have:

\begin{align*}
    &(A^{-1}(y_1' - b) - x^{(k)})^\intercal (A^{(k)})^{-1} (A^{-1}(y_1' - b) - x^{(k)}) \le 1 \\
    \Rightarrow \ & (A^{-1}y_1' - A^{-1}b - x^{(k)})^\intercal \textcolor{orange}{(A^{(k)})^{-1}} (A^{-1}y_1' - A^{-1}b - x^{(k)}) \le 1 \\
    \Rightarrow \ & \left[((y_1')^\intercal (A^{-1})^\intercal\textcolor{orange}{(A^{(k)})^{-1}} - b^\intercal(A^{-1})^\intercal\textcolor{orange}{(A^{(k)})^{-1}} - (x^{(k)})^\intercal\textcolor{orange}{(A^{(k)})^{-1}}) \right]
    (A^{-1}y_1' - A^{-1}b - x^{(k)}) \le 1 \\
\end{align*}
\end{comment}



\subsection*{(4)}

\end{document} 