\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} 
\usepackage[]{amssymb} 
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[thinc]{esdiff}
\newtheorem*{theorem}{Theorem}

\title{Optimization Algorithms: HW0}
\author{Lo Chun, Chou \\ R13922136}
\date\today

\begin{document}
\setlength{\parindent}{0pt}
\maketitle 

\section*{1}
\subsection*{(1)}

To show that the optimization problem defining $w^\natural$ is convex, 
we need to show that both the objective function and the constraint set are convex.
\bigskip

Claim: The objective function 
$g (w) := \frac{1}{2n} \sum_{i = 1}^n ( y_i - \langle x_i, w \rangle )^2$ is convex,
and the constraint set $\mathbb{R}^d$ is also convex.

\bigskip
To prove that $g(w)$ is convex, we would use the theorem that:

\begin{theorem}
    \footnote{S. Boyd and L. Vandenberghe, \textit{Convex Optimization}, Cambridge University Press, 2004, pp. 71.}
    Assume that a function $f$ is twice differentiable, then $f$ is convex $\Leftrightarrow$ 
    $\mathrm{dom} f$ is convex and its Hessian is positive semidefinite.
\end{theorem}

To check that if $g(w)$ is twice differentiable, we first convert the original definition into a matrix-vector form, by letting:

\begin{equation*}
    X = \begin{bmatrix}
        x_1^\intercal  \\
        x_2^\intercal  \\
        \vdots \\
        x_n^\intercal 
    \end{bmatrix} \in \mathbb{R}^{n \times d}
    \quad \text{and} \quad
    y = \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{bmatrix} \in \mathbb{R}^n
    \quad \text{and} \quad
    w = \begin{bmatrix}
        w_1 \\
        w_2 \\
        \vdots \\
        w_d
    \end{bmatrix} \in \mathbb{R}^d
\end{equation*}

Then, we'll get:

\begin{equation*}
    \begin{split}
    g(w) 
        &= \frac{1}{2n} \sum_{i = 1}^n ( y_i - x_i^\intercal  w)^2 \\
        &= \frac{1}{2n} \sum_{i = 1}^n\left[y_i^2 - 2y_ix_i^\intercal w + (x_i^\intercal w)^2 \right]\\
        &= \frac{1}{2n} (y^\intercal y - 2y^\intercal Xw + (Xw)^\intercal Xw)\\
        &= \frac{1}{2n} (y^\intercal y - 2w^\intercal X^\intercal y + w^\intercal X^\intercal Xw)
    \end{split}
\end{equation*}

Differentiate w.r.t. $w$:

\begin{align*}
    \nabla g(w) 
        &= \diffp{}{w} \left[\frac{1}{2n} (y^\intercal y - 2w^\intercal X^\intercal y + w^\intercal X^\intercal Xw) \right]\\
        &= \frac{1}{2n} \left[0 - 2X^\intercal y + 2X^\intercal Xw\right] \\
        &= - \frac{1}{n} X^\intercal y + \frac{1}{n} X^\intercal Xw \tag{1}
\end{align*} 

Then the second derivative:

\begin{align*}
    \nabla^2 g(w) 
        &= \diffp{}{w} \left[- \frac{1}{n} X^\intercal y + \frac{1}{n} X^\intercal Xw \right]\\
        &= \frac{1}{n} X^\intercal X \tag{2}
\end{align*}

Since $\frac{1}{n} X^\intercal X$ does not depend on $w$, it is a constant matrix, 
and therefore the second derivative exists at each point in $\mathrm{dom} f$.
We can now check the conditions of the theorem.
\bigskip

The domain of $g(w)$ is $\mathbb{R}^d$, which is convex.
\footnote{S. Boyd and L. Vandenberghe, \textit{Convex Optimization}, Cambridge University Press, 2004, pp. 27.}
\bigskip

For any $v \in \mathbb{R}^d$, we have:

\begin{equation*}
    \begin{split}
        v^\intercal  \nabla^2 g(w) v 
            &= v^\intercal  \frac{1}{n} X^\intercal X v \\
            &= \frac{1}{n} (Xv)^\intercal  Xv \\
            &= \frac{1}{n} \|Xv\|_2^2 \ge 0
    \end{split}
\end{equation*}

Thus, the Hessian of $g(w)$ is positive semidefinite, and $g(w)$ is convex. 
\bigskip

Finally, the constraint set $\mathbb{R}^d$ is also convex, as shown above,
we can conclude that the optimization problem defining $w^\natural$ is convex. $\square$

\subsection*{(2)}

For $t=1$, we have:

\begin{equation*}
    w_2 = w_1 - \left( \nabla^2 g ( w_1 ) \right)^{-1} \nabla g ( w_1 ), 
    \qquad \text{where } w_1 = 0 \in \mathbb{R}^d \tag{*}
\end{equation*}
\newpage

To get $w_2$, we need to calculate $\nabla g ( w_1 ), \ \nabla^2 g ( w_1 )$,
from $(1)$ in the previous question, we have:

\begin{equation*}
    \begin{split}
        \nabla g ( w_1 ) 
            &= - \frac{1}{n} X^\intercal y + \frac{1}{n} X^\intercal Xw_1 \\
            &= - \frac{1}{n} X^\intercal y
    \end{split}
\end{equation*}

And from $(2)$ in the previous question, we have:

\begin{equation*}
    \nabla^2 g ( w_1 ) = \frac{1}{n} X^\intercal X
\end{equation*}
    
Plugging back into $(*)$, we get:

\begin{equation*}
    \begin{split}
        w_2 
            &= w_1 - \left( \nabla^2 g ( w_1 ) \right)^{-1} \nabla g ( w_1 ) \\
            &= 0 - \left( \frac{1}{n} X^\intercal X \right)^{-1} \left( - \frac{1}{n} X^\intercal y \right) \\
            &= 0 + n (X^\intercal X)^{-1} \frac{1}{n} X^\intercal y \\
            &= (X^\intercal X)^{-1} X^\intercal y
    \end{split} 
\end{equation*}

To show that $w_2 = w^\natural$, observe that $\nabla g(w^\natural) = 0$, using $(1)$ in the previous question, we have: 

\begin{equation*}
    \begin{split}
        \nabla g(w^\natural) 
            &= - \frac{1}{n} X^\intercal y + \frac{1}{n} X^\intercal Xw^\natural \\
            &= - \frac{1}{n} X^\intercal y + \frac{1}{n} X^\intercal Xw^\natural = 0
    \end{split}
\end{equation*}

Reorder and simplify the terms, we get:

\begin{equation*}
    X^\intercal Xw^\natural = X^\intercal y
\end{equation*}


Since we can only show that $X^\intercal X$ is positive semi-definite, 
we cannot guarantee that the inverse $(X^\intercal X)^{-1}$ exists, 
so we should take the Moore-Penrose pseudo-inverse, 
which is uniquely defined for any matrix:

\begin{equation*}
    w^\natural = (X^\intercal X)^+ X^\intercal y
\end{equation*}

\section*{2}
\subsection*{(1)}

Since $y_1,\dots ,y_n$ are random variables that satisfy:

\begin{equation*}
    \mathsf{P} ( y_i = 1 ) = 1 - \mathsf{P} ( y_i = 0 ) = \frac{1}{1 + \mathrm{e}^{- \langle x_i, \theta^\natural \rangle}} 
\end{equation*}

We knew that the probability of $\mathsf{P} ( y_i = 0 )$ is:

\begin{equation*}
    \begin{split}
        \mathsf{P} ( y_i = 0 ) 
        &= \frac{1}{1 + \mathrm{e}^{\langle x_i, \theta^\natural \rangle}} \\
        &= \frac{\mathrm{e}^{-\langle x_i, \theta^\natural \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta^\natural \rangle}} 
    \end{split}
\end{equation*}

We can write the pmf of given $x_i, \theta^{\natural}$, observed $y_i$ as:

\begin{equation*}
    p(y_i | x_i, \theta^{\natural}) = \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} \right)^{y_i} \left( \frac{\mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta^{\natural} \rangle}} \right)^{1 - y_i}
\end{equation*}

Using the pmf, we can get the likelihood function, which can be written as:

\begin{equation*}
    l(\theta) = \prod_{i=1}^n p(y_i | x_i, \theta)
    \footnote{Robert V. Hogg, Elliot A. Tanis, Dale Zimmerman, \textit{Probability and Statistical Inference}, 9th ed., Pearson Education, 2015, p. 258-259.}
\end{equation*}

We can further derive the $\log$-likelihood:

\begin{align*}
    \log l(\theta) 
    &= \log \left[\prod_{i=1}^n \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right)^{y_i} \left( \frac{\mathrm{e}^{-\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right)^{1 - y_i} \right] \\
    &= \sum_{i=1}^n \left[ y_i \log \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right) + (1 - y_i) \log \left( \frac{\mathrm{e}^{-\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right) \right] \tag{*}
\end{align*}

The terms in the above equation can be simplified:

\begin{equation*}
    \begin{split}
        y_i \log \left( \frac{1}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right) 
        &= y_i \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right)^{-1} \\
        &= -y_i \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right)
    \end{split}
\end{equation*}

\begin{equation*}
    \begin{split}
        (1 - y_i) \log \left( \frac{\mathrm{e}^{-\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right) 
        &= (1 - y_i) \log \left( \mathrm{e}^{-\langle x_i, \theta \rangle} \right) - (1 - y_i) \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right)\\
        &= -\langle x_i, \theta \rangle(1 - y_i) - \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) + y_i \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right)
    \end{split}
\end{equation*}

Plugging back into $(*)$, we get:

\begin{equation*}
    \begin{split}
        \log l(\theta) 
        &= \sum_{i=1}^n \left[ -y_i \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) - \langle x_i, \theta \rangle(1 - y_i) - \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) + y_i \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) \right] \\
        &= \sum_{i=1}^n \left[ - \langle x_i, \theta \rangle(1 - y_i) - \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) \right]
    \end{split}
\end{equation*}

We can find the maximum likelihood estimator $\hat{\theta}_n$ by maximizing the $\log$-likelihood function,
which is equivalent to find the minimum of the negative $\log$-likelihood function.
\bigskip

Thus, we should compute the gradient of the negative $\log$-likelihood w.r.t. $\theta$, set to $0$ and solve for $\theta$
\footnote{Deisenroth, Marc Peter, Faisal, A. Aldo, Ong, Cheng Soon, \textit{Mathematics for Machine Learning}, Cambridge University Press, 2020, pp. 351.}
:

\begin{equation*}
    \begin{split}
        \nabla \left( - \log l(\theta) \right) 
        &= \nabla \left( - \sum_{i=1}^n \left[ - \langle x_i, \theta \rangle(1 - y_i) - \log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right) \right] \right) \\
        &= \nabla \left( \sum_{i=1}^n \langle x_i, \theta \rangle(1 - y_i)\right) + \nabla \left(\sum_{i=1}^n\log \left( 1 + \mathrm{e}^{-\langle x_i, \theta \rangle} \right)\right) \\
        &= \sum_{i=1}^n \left[ x_i(1 - y_i) - x_i \frac{\mathrm{e}^{-\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta \rangle}} \right] = 0 \\
    \end{split}
\end{equation*}

If $\theta^\natural$ is given by:

\begin{equation*}
    \hat{\theta}_n \in \underset{\theta \in \mathbb{R}^p}{\operatorname{argmin}} L ( \theta ), \quad L (\theta) := \frac{1}{n} \sum_{i = 1}^n \log \left( 1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle} \right)
\end{equation*}

Then $\hat{\theta}_n$ should also satisfy $\nabla L(\theta) = 0$, so we have:

In order to calculate it, we can use the fact that the term inside the summation is of the form:

\begin{equation*}
    \log(1 + e^z)
\end{equation*}

So taking the derivative w.r.t. $z$ gives:

\begin{equation*}
    \frac{d}{dz} \log(1 + e^z) = \frac{e^z}{1 + e^z}
\end{equation*}

Therefore we have $z = -2(y_i - \frac{1}{2})\langle x_i, \theta \rangle$, and combined using the chain rule will give:

\begin{align*}
    \nabla L(\theta)
    &= \nabla \left( \frac{1}{n} \sum_{i = 1}^n \log \left( 1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle} \right) \right) \\
    &= \frac{1}{n} \sum_{i = 1}^n \nabla \left( \log \left( 1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle} \right) \right) \\
    &= \frac{1}{n} \sum_{i = 1}^n \frac{1}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}} \cdot \nabla \left( -2(y_i - \frac{1}{2})\langle x_i, \theta \rangle \right) \\
    &= \frac{1}{n} \sum_{i = 1}^n \frac{1}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}} \cdot -2(y_i - \frac{1}{2}) x_i = 0
\end{align*}

Simplify:

\begin{align*}
    \sum_{i = 1}^n \frac{(1 - y_i)x_i}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}} = 0 \tag{1}
\end{align*}

Observe that when $y_i = 1$, the term in the summation will become $0$, 
and if $y_i = 0$, the term will become:

\begin{equation*}
    \frac{x_i}{1 + e^{\langle x_i, \theta \rangle}}
\end{equation*}

Thus, we can rewrite $(1)$ as:

\begin{equation*}
    n \cdot \frac{\mathrm{e}^{-\langle x_i, \theta^\natural \rangle}}{1 + \mathrm{e}^{-\langle x_i, \theta^\natural \rangle}} \cdot \left( \frac{x_i}{1 + e^{\langle x_i, \theta \rangle}} \right) = 0
\end{equation*}

\subsection*{(2)}

To show that the optimization problem defining the maximum-likelihood estimator is convex,
we need to show that both the objective function and the constraint set are convex.
\bigskip

As in $1.(1)$, we knew that the constraint set $\mathbb{R}^p$ is convex,
therefore, we only need to check the convexity of the objective function.
\bigskip

Claim: The objective function 
$L (\theta) := \frac{1}{n} \sum_{i = 1}^n \log \left( 1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle} \right)$ is convex.

Following the same steps in $1.(1)$, we first differentiate $L (\theta)$ w.r.t. $\theta$:

\begin{align*}
    \nabla L(\theta) 
    &= \frac{1}{n} \sum_{i = 1}^n \frac{1}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}} \cdot -2(y_i - \frac{1}{2}) x_i \cdot \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle} \\
    &= \frac{1}{n} \sum_{i = 1}^n \frac{x_i(1 - y_i)\mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}}{1 + \mathrm{e}^{-2(y_i - \frac{1}{2})\langle x_i, \theta \rangle}}
\end{align*}

To make the equation more readble, 
we can represent $z_i = 2(y_i - \frac{1}{2})\langle x_i, \theta \rangle$, 
so that $\nabla L(\theta)$ is equivalent to:

\begin{align*}
    \nabla L(\theta) 
    &= \frac{1}{n} \sum_{i = 1}^n \frac{x_i(1 - y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}}
\end{align*}

In order to calculate the Hessian, 
we first calculate some of the terms:

\begin{align*}
    \frac{d}{d\theta} z_i = 2(y_i - \frac{1}{2})x_i
\end{align*}

\begin{align*}
    \frac{d}{d\theta}e^{-z_i} = -2(y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i}
\end{align*}

Then we'll have:

\begin{align*}
    \frac{d}{d\theta} \left( \frac{(1 - y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}} \right)
    &= \frac{\frac{d}{d\theta}\left((1 - y_i)\mathrm{e}^{-z_i} \right) \cdot (1 + \mathrm{e}^{-z_i}) - (1 - y_i)\mathrm{e}^{-z_i} \cdot \frac{d}{d\theta}( 1 + \mathrm{e}^{-z_i})}{(1 + \mathrm{e}^{-z_i})^2} \\
    &= \frac{-2(1 - y_i)(y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i} (1 + \mathrm{e}^{-z_i}) + 2 (1 - y_i)\mathrm{e}^{-z_i} (y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i}}{(1 + \mathrm{e}^{-z_i})^2} \\
    &= 2(1 - y_i)(y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i}\frac{ (-1 - \mathrm{e}^{-z_i} + \mathrm{e}^{-z_i})}{(1 +\mathrm{e}^{-z_i})^2} \\
    &= \frac{-2(1 - y_i)(y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2} \\
\end{align*}

Getting back to the Hessian, we have:

\begin{align*}
    \nabla^2 L(\theta)
    &= \frac{1}{n} \sum_{i = 1}^n \frac{d}{d\theta} \left[ x_i \left( \frac{(1 - y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}} \right) \right] \\
    &= \frac{1}{n} \sum_{i = 1}^n x_i \frac{-2(1 - y_i)(y_i - \frac{1}{2})x_i\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2} \\
    &= \frac{1}{n} \sum_{i = 1}^n x_i x_i^\intercal  \frac{-2(1 - y_i)(y_i - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2} \tag{1}
\end{align*}

Since $(1 +\mathrm{e}^{-z_i})^2$ is strictly positive, the Hessian exists for all point in $\mathbb{R}^p$.
Therefore, $L(\theta)$ is twice differentiable.
\bigskip

Since we knew that the domain of $L(\theta)$ is convex, 
we only need to check if the Hessian is positive semidefinite to prove that $L(\theta)$ is convex.
\bigskip

By $(1)$, for any $v \in \mathbb{R}^p$, we have:

\begin{align*}
    v^\intercal  \nabla^2 L(\theta) v
    &= \frac{1}{n} \sum_{i = 1}^n v^\intercal  x_i x_i^\intercal  \frac{-2(1 - y_i)(y_i - \frac{1}{2})\mathrm{e}^{-z_i}}{(1 +\mathrm{e}^{-z_i})^2} v \\
    &= \frac{1}{n} \sum_{i = 1}^n \frac{-2(1 - y_i)(y_i - \frac{1}{2})v^\intercal  x_i x_i^\intercal  v}{(1 +\mathrm{e}^{-z_i})^2} \\
\end{align*}

For the denominator, $(1 +\mathrm{e}^{-z_i})^2 > 0$, 
and for the coefficient, $-2(1 - y_i)(y_i - \frac{1}{2})$, 
since $y_i \in \{0, 1\}$, we have:

\begin{align*}
    -2(1 - y_i)(y_i - \frac{1}{2}) \ge 0
\end{align*}

Last, we have $v^\intercal  x_i x_i^\intercal  v$, this is equivelent to $(v^\intercal x_i)^2$, which is non-negative.

Therefore, we have:

\begin{align*}
    \frac{1}{n} \sum_{i = 1}^n \frac{-2(1 - y_i)(y_i - \frac{1}{2})v^\intercal  x_i x_i^\intercal  v}{(1 +\mathrm{e}^{-z_i})^2} \ge 0
\end{align*}

Thus the Hessian is positive semidefinite, and $L(\theta)$ is convex. $\square$

\subsection*{(3)}

Let: 
\begin{align*}
    X = \begin{bmatrix}
        x_1^\intercal  \\
        x_2^\intercal  \\
        \vdots \\
        x_n^\intercal 
    \end{bmatrix}
    \in \mathbb{R}^{n \times p}
    \qquad
    y = \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{bmatrix}
    \in \mathbb{R}^n
\end{align*}

By the previous subproblem, we have:

\begin{align*}
    \nabla L(\theta^{\natural}) 
    &= \frac{1}{n} \sum_{i = 1}^n \frac{x_i(1 - y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}} 
    \qquad \text{where } z_i = 2(y_i - \frac{1}{2})\langle x_i, \theta^{\natural} \rangle
\end{align*}

Thus, to show that $\nabla L ( \theta^\natural ) = - \frac{1}{n} X^\intercal  (y - \mathsf{E} [ y ] )$,  
it is equivalent to prove:

\begin{align*}
    \sum_{i = 1}^n \frac{x_i(1 - y_i)\mathrm{e}^{-z_i}}{1 + \mathrm{e}^{-z_i}} = X^\intercal  (y - \mathsf{E} [ y ])
\end{align*}


=== could be wrong ===
\bigskip

And since $\theta^{\natural}$ is the true parameter, 
this implies that it would minimize the error function $L(\theta)$, 
which is equivalent to satisfy:

\begin{align*}
    \nabla L(\theta^{\natural}) = 0
\end{align*}









\end{document}