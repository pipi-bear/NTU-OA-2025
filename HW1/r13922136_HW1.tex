\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} 
\usepackage[]{amssymb} 
\usepackage{amsmath}
\DeclareMathOperator{\domain}{dom}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{comment}
\usepackage[dvipsnames]{xcolor}
\usepackage[thinc]{esdiff}
\usepackage{tcolorbox}
\newtheorem*{theorem}{Theorem}

\title{Optimization Algorithms: HW1}
\author{Lo Chun, Chou \\ R13922136}
\date\today

\begin{document}
\setlength{\parindent}{0pt}
\maketitle 

\tcbset{
    greenbox/.style={
        colback=SpringGreen!20,
        colframe=SpringGreen!80,
        coltitle=black,
        sharp corners
    },
    bluebox/.style={
        colback=SkyBlue!20,
        colframe=SkyBlue!80,
        coltitle=black,
        sharp corners
    },
    yellowbox/.style={
        colback=yellow!10,
        colframe=yellow!80,
        coltitle=black,
        sharp corners
    }
}

\section*{1}

\section*{2}

\subsection*{(1)}

Given a twice differentiable function $\varphi: \mathbb{R}^d \to [- \infty, \infty]$, 
assume that it is logarithmically homogeneous, 
then by the definition, the following holds:

\begin{align*}
    \varphi ( \gamma x ) = \varphi (x) - \log \gamma, \quad \forall x \in \mathbb{R}^d, \gamma > 0 \tag{1}
\end{align*}

\textcolor{blue}{\underline{Claim: $\langle \nabla \varphi (x), x \rangle = - 1$ } }
\bigskip

To derive the first equation, we first define the following:

\begin{align*}
    F(\gamma) = \textcolor{orange}{\varphi (\gamma x)}
\end{align*}

Then the original equation $(1)$ would become:

\begin{align*}
    F(\gamma) = \textcolor{Green}{\varphi (x) - \log \gamma}
\end{align*}

Taking the derivative w.r.t. $\gamma$ on both sides, we get:

\begin{align*}
    \frac{dF}{d\gamma} &= \frac{d}{d\gamma} \textcolor{orange}{\varphi (\gamma x)} = \nabla \varphi (\gamma x) \cdot x = \langle \nabla \varphi (\gamma x), x \rangle \tag{2}\\
    \frac{dF}{d\gamma} &= \frac{d}{d\gamma} (\textcolor{Green}{\varphi (x) - \log \gamma}) = - \frac{1}{\gamma} \tag{3}
\end{align*}

Thus by $(2)$ and $(3)$, we have:

\begin{align*}
    \langle \nabla \varphi (\gamma x), x \rangle = - \frac{1}{\gamma}
\end{align*}
Then by plugging in $\gamma = 1$, we have:

\begin{align*}
    \langle \nabla \varphi (x), x \rangle = - 1 \qquad \square
\end{align*}

\textcolor{blue}{\underline{Claim: $\nabla \varphi (x) = - \nabla^2 \varphi ( x ) x$ } }
\bigskip

From the previous part, we have:

\begin{align*}
    \nabla \varphi (x)^T x = - 1
\end{align*}

Compute the gradient of both sides, for the left hand side, we have:

\begin{align*}
    \textcolor{orange}{\nabla(}\nabla \varphi (x)^T x\textcolor{orange}{)}
    &= \textcolor{orange}{\nabla (}\nabla \varphi (x)\textcolor{orange}{)}^T x + \nabla \varphi (x)^T \textcolor{orange}{\nabla} x  \\
    &= \nabla^2 \varphi (x) x + \nabla \varphi (x)^T \nabla x  \\
\end{align*}

For the right hand side, we have:

\begin{align*}
    \nabla (- 1) = 0
\end{align*}

Thus we have:

\begin{align*}
    &\nabla^2 \varphi (x) x + \nabla \varphi (x)^T \nabla x = 0 \\
    \Rightarrow &\nabla \varphi (x)^T \nabla x = - \nabla^2 \varphi (x) x \\
    \Rightarrow &\nabla \varphi (x)^T I_d = - \nabla^2 \varphi (x) x \\
    \Rightarrow &\nabla \varphi (x) = - \nabla^2 \varphi (x) x \qquad \square
\end{align*}

\textcolor{blue}{\underline{Claim: $\langle x, \nabla^2 \varphi (x) x \rangle = 1$ } }
\bigskip

From the previous part, we have:

\begin{align*}
    \nabla \varphi (x) = - \nabla^2 \varphi (x) x
\end{align*}

Multiply both sides by $x^T$, we have:

\begin{align*}
    x^T \nabla \varphi (x) = - x^T \nabla^2 \varphi (x) x
\end{align*}

Which is equivalent to the following by using $\langle \nabla \varphi (x), x \rangle = - 1$:

\begin{align*}
    \langle x, \nabla^2 \varphi (x) x \rangle = - \langle x, \nabla \varphi (x) \rangle = (-1) \times (-1) = 1 \qquad \square
\end{align*}

\subsection*{(2)}

Suppose that $\varphi: \mathbb{R}^d \to [- \infty, \infty]$ is a twice differentiable function, 
and is strictly convex and logarithmically homogeneous, then the following holds by the definition:

\begin{align*}
    &\nabla^2 \varphi (x) > 0 \quad \forall x \in \mathbb{R}^d \\
    &\varphi ( \gamma x ) = \varphi (x) - \log \gamma, \quad \forall x \in \mathbb{R}^d, \gamma > 0.
\end{align*}

Also, we have the following properties from the previous subsection:

\begin{align*}
    \langle \nabla \varphi (x), x \rangle = - 1 \tag{1}\\
    \nabla \varphi (x) = - \nabla^2 \varphi ( x ) x \tag{2}\\
    \langle x, \nabla^2 \varphi (x) x \rangle = 1 \tag{3}
\end{align*}

\textcolor{blue}{\underline{Claim: $\nabla^2 \varphi ( x ) \geq \nabla \varphi ( x ) \left( \nabla \varphi (x) \right)^T , \quad \forall x \in \operatorname{dom}\, \varphi $} }
\bigskip

The claim is equivalent to proving that:

\begin{align*}
    \nabla^2 \varphi ( x ) - \nabla \varphi ( x ) \left( \nabla \varphi (x) \right)^T \succeq 0
\end{align*}

where $\succeq 0$ denotes positive semidefinite.

Let $z$ be any vector in $\mathbb{R}^d$, then we have:

\begin{align*}
    z^T \left( \nabla^2 \varphi ( x ) - \nabla \varphi ( x ) \left( \nabla \varphi (x) \right)^T \right) z
    &= z^T \nabla^2 \varphi ( x ) z - z^T \nabla \varphi ( x ) \left( \nabla \varphi (x) \right)^T z \\
    &= z^T \nabla^2 \varphi ( x ) z - \left( \nabla \varphi ( x )^T z \right)^2 \tag{*}\\
\end{align*}

\textbf{\underline{Case 1: $x = z$}}
\bigskip

If $x = z$, then $(*)$ becomes the following using \textcolor{orange}{$(1)$} and \textcolor{Green}{$(2)$}:

\begin{align*}
    z^T \left( \nabla^2 \varphi ( x ) - \nabla \varphi ( x ) \left( \nabla \varphi (x) \right)^T \right) z 
    &= z^T \textcolor{Green}{\nabla^2 \varphi ( x ) x} - (\textcolor{orange}{\langle \nabla \varphi ( x ), x \rangle})^2 \\
    &= x^T \textcolor{Green}{(- \nabla \varphi ( x ))} - (\textcolor{orange}{-1})^2 \\
    &= x^T (- \nabla \varphi ( x )) - 1 \\
    &= - \textcolor{orange}{\langle \nabla \varphi ( x ), x \rangle} - 1 \\
    &= - \textcolor{orange}{(-1)} - 1 \\
    &= 0 \geq 0 
\end{align*}

\textbf{\underline{Case 2: $x \neq z$}}
\bigskip

Using \textcolor{Green}{$(2)$} to replace $\nabla \varphi (x)$ with $- \nabla^2 \varphi ( x ) x$ in $(*)$, 
and using the fact that $(\nabla^2 \varphi ( x ))^T = \nabla^2 \varphi ( x )$ (the Hessian is symmetric):

\begin{align*}
    z^T \left( \nabla^2 \varphi ( x ) - \nabla \varphi ( x ) \left( \nabla \varphi (x) \right)^T \right) z 
    &= z^T \nabla^2 \varphi ( x ) z - \left( \textcolor{Green}{\nabla \varphi ( x )}^T z \right)^2 \\
    &= z^T \nabla^2 \varphi ( x ) z - \left( \textcolor{Green}{(- \nabla^2 \varphi ( x ) x)}^T z \right)^2 \\
    &= z^T \nabla^2 \varphi ( x ) z - \left( - \underbrace{x^T}_{A^T} \underbrace{(\nabla^2 \varphi ( x ))^T z }_{B} \right)^2 \\
    &= z^T \nabla^2 \varphi ( x ) z - \underbrace{[(\nabla^2 \varphi ( x ))^T z]^T x x^T [(\nabla^2 \varphi ( x ))^T z]}_{B^TAA^TB} \\
    &= z^T \nabla^2 \varphi ( x ) z - [\textcolor{blue}{x^T (\nabla^2 \varphi ( x ))^T z}]^T [\textcolor{blue}{x^T (\nabla^2 \varphi ( x ))^T z}] \\
    &= z^T \nabla^2 \varphi ( x ) z - ||\textcolor{blue}{x^T (\nabla^2 \varphi ( x ))^T z}||^2 \\
    &= z^T \nabla^2 \varphi ( x ) z - ||x^T (\nabla^2 \varphi ( x )) z||^2 
\end{align*}

Let $H = \nabla^2 \varphi ( x )$, then the above expression is equivalent to:

\begin{align*}
    z^T H z - (x^T H z)^2
\end{align*}

Let's first check that we can define the function 
\begin{align*}
    h: \mathbb{R}^d \times  \mathbb{R}^d \to \mathbb{R}, \quad h(u, v) = u^T H v
\end{align*}

as the inner product on $\mathbb{R}^d$.
\footnote{H. Amann and J. Escher, \textit{Analysis I}, 1st ed., Birkhäuser Basel, 2005, p.~153.}
\bigskip

Using the fact that we assumed that $\nabla^2 \varphi (x) > 0$, so $H$ is positive definite, 
thus by theorem
\footnote{``Square root of a matrix'', Wikipedia, \url{https://en.wikipedia.org/wiki/Square_root_of_a_matrix}}, 
there exists a one and only one positive definite matrix $H^{1/2}$ (also symmetric) such that $H = H^{1/2} H^{1/2}$.

\begin{itemize}
    \item \textbf{Symmetry:} For any $u, v \in \mathbb{R}^d$, we have:
    \begin{align*}
        h(u, v) 
        &= u^T H v \\
        &= u^T H^{1/2} H^{1/2} v \\
        &= (H^{1/2} u)^T (H^{1/2} v) \\
        &= (H^{1/2} v)^T (H^{1/2} u) \\
        &= v^T H^{1/2} H^{1/2} u \\
        &= v^T H u \\
        &= h(v, u)
    \end{align*}
    \item \textbf{Linearity:} For any $\lambda, \mu \in \mathbb{R}$ and $t, u, v \in \mathbb{R}^d$, we have:
    \begin{align*}
        h(t, \lambda u + \mu v) 
        &= t^T H (\lambda u + \mu v) \\
        &= t^T H (\lambda u + \mu v) \\
        &= t^T H (\lambda u) + t^T H (\mu v) \\
        &= \lambda t^T H u + \mu t^T H v \\
        &= \lambda h(t, u) + \mu h(t, v)
    \end{align*}
    \item \textbf{Positive definiteness:} For any $u \in \mathbb{R}^d$, we have:
    \begin{align*}
        h(u, u) = u^T H u > 0 \quad \text{since $H$ is positive definite}
    \end{align*}
\end{itemize}
\footnote{I later found that we have 
"A function $\langle \cdot, \cdot \rangle : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ 
is an inner product on $\mathbb{R}^n$ if and only if there exists a symmetric positive-definite matrix $\mathbf{M}$ 
such that $\langle x, y \rangle = x^\top \mathbf{M} y$ for all $x, y \in \mathbb{R}^n$."
on ``Inner product space'', Wikipedia, \url{https://en.wikipedia.org/wiki/Inner_product_space}}

Therefore, we have:

\begin{align*}
    z^T H z - (x^T H z)^2 
    &= \langle z, z \rangle_H - \langle x, z \rangle_H^2 \\
\end{align*}

Using the Cauchy-Schwarz inequality
\footnote{H. Amann and J. Escher, \textit{Analysis I}, 1st ed., Birkhäuser Basel, 2005, p.~154.}:
\bigskip

\begin{tcolorbox}[greenbox, title = Cauchy-Schwarz inequality]
    Let $(E, (\cdot \mid \cdot))$ be an inner product space. Then  
    \begin{align*}
        |(x \mid y)|^2 \leq (x \mid x)(y \mid y), \qquad x,y \in E
    \end{align*}
\end{tcolorbox}

We can derive the later equation using the fact that:

\begin{align*}
    \langle x, x \rangle_H = x^T H x = x^T \nabla^2 \varphi ( x ) x = 1
\end{align*}

(this is because property $(3)$)

So we have:

\begin{align*}
    \langle x, z \rangle_H^2 
    &\leq \langle x, x \rangle_H \langle z, z \rangle_H \\
    &= 1 \times \langle z, z \rangle_H \\
    &= \langle z, z \rangle_H
\end{align*}

Thus we have:

\begin{align*}
    z^T H z - (x^T H z)^2 \geq 0 \qquad \square
\end{align*}

\subsection*{(3)}

We need to prove the following equivalence:

\begin{align*}
    & \text{(1)} \quad e^{-\varphi(x)} \text{ is concave} \\
    \iff \quad & \text{(2)} \quad \varphi(y) \geq \varphi(x) - \log(1 - \langle \nabla \varphi(x), y - x \rangle), \quad \forall x, y \in \operatorname{dom}(\varphi) \\
    \iff \quad & \text{(3)} \quad \nabla^2 \varphi(x) \succeq \nabla \varphi(x) \nabla \varphi(x)^\top, \quad \forall x \in \operatorname{dom}(\varphi)
\end{align*}

\textbf{\underline{(1) $\implies$ (2)}}
\bigskip

Let $f: \mathbb{R}^d \to \mathbb{R}$ be defined as $f(x) = e^{-\varphi(x)}$.
\bigskip

Suppose that $f(x) = e^{-\varphi(x)}$ is concave, then by the definition of concavity
\footnote{Y. Nesterov, \textit{Introductory Lectures on Convex Optimization: A Basic Course}, 1st ed., Springer, New York, NY, 2004, p.~52.}:

\begin{tcolorbox}[bluebox, title = Convex]
    A continuously differentiable function $f(x)$ is called convex on $\mathbb{R}^n$ if for any $x, y \in \mathbb{R}^n$, we have:
    \begin{align*}
        f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle
    \end{align*}
    If $-f(x)$ is convex, then $f(x)$ is concave.
\end{tcolorbox}

this means that our assumption is equivalent to saying that $-e^{-\varphi(x)}$ is convex. 
Let $g(x) = -f(x) = -e^{-\varphi(x)}$ a convex function, 
using the fact that:
\begin{align*}
    \nabla g(x) = \frac{d}{dx} (-e^{-\varphi(x)}) = e^{-\varphi(x)} \nabla \varphi(x)
\end{align*}

we have the following:
\bigskip

For any $x, y \in \mathbb{R}^d$:

\begin{align*}
    &g(y) \geq g(x) + \langle \nabla g(x), y - x \rangle \\
    \Rightarrow \ &-e^{-\varphi(y)} \geq -e^{-\varphi(x)} + \langle e^{-\varphi(x)} \nabla \varphi(x), y - x \rangle \\
    \Rightarrow \ &e^{-\varphi(y)} \leq e^{-\varphi(x)} - e^{-\varphi(x)} \langle \nabla \varphi(x), y - x \rangle \\
    \Rightarrow \ &e^{-\varphi(y)} \leq e^{-\varphi(x)} (1 - \langle \nabla \varphi(x), y - x \rangle) \\
    \Rightarrow \ &- \varphi(y) \leq -\varphi(x) + \log (1 - \langle \nabla \varphi(x), y - x \rangle) \\
    \Rightarrow \ &\varphi(y) \geq \varphi(x) - \log (1 - \langle \nabla \varphi(x), y - x \rangle)
\end{align*}

\textbf{\underline{(2) $\implies$ (3)}}
\bigskip

Suppose $(2)$ holds, so we have:

\begin{align*}
    \varphi(y) \geq \varphi(x) - \log (1 - \langle \nabla \varphi(x), y - x \rangle), \quad \forall x, y \in \operatorname{dom}(\varphi)
\end{align*}

By plugging in $y = x + h \ (h = y - x)$, with $||h|| \rightarrow 0$, we have:

\begin{align*}
    \varphi(x + h) \geq \varphi(x) - \log (1 - \langle \nabla \varphi(x), h \rangle) \tag{1}
\end{align*}

Then by using the second-order approximation
\footnote{Y. Nesterov, \textit{Introductory Lectures on Convex Optimization: A Basic Course}, 1st ed., Springer, New York, NY, 2004, p.~19.}:

\begin{tcolorbox}[bluebox, title = Second-order approximation]
    Let $f$ be twice differentiable at $\bar{x}$. Then
    \begin{align*}
        f(y) = f(\bar{x}) + \langle \nabla f(\bar{x}), y - \bar{x} \rangle + \frac{1}{2} \langle\nabla^2 f(\bar{x}) (y - \bar{x}), y - \bar{x} \rangle + o(||y - \bar{x}||^2)
    \end{align*}
\end{tcolorbox}

Since $\varphi$ is twice differentiable on its domain, we have:

\begin{align*}
    \varphi(x + h) = \varphi(x) + \langle \nabla \varphi(x), h \rangle + \frac{1}{2} \langle\nabla^2 \varphi(x) h, h \rangle + o(||h||^2) \tag{2}
\end{align*}

Combining $(1)$ and $(2)$, we have:

\begin{align*}
    &\textcolor{orange}{\varphi(x)} + \langle \nabla \varphi(x), h \rangle + \frac{1}{2} \langle\nabla^2 \varphi(x) h, h \rangle + o(||h||^2) 
    \geq \textcolor{orange}{\varphi(x)} - \log (1 - \langle \nabla \varphi(x), h \rangle) \\
    \Rightarrow \ &\langle \nabla \varphi(x), h \rangle + \frac{1}{2} \langle\nabla^2 \varphi(x) h, h \rangle + o(||h||^2)  \geq - \textcolor{Green}{\log (1 - \langle \nabla \varphi(x), h \rangle)}  \\
    \Rightarrow \ &\langle \nabla \varphi(x), h \rangle + \frac{1}{2} \langle\nabla^2 \varphi(x) h, h \rangle + o(||h||^2)  \geq - \textcolor{Green}{(-\sum_{n=1}^{\infty} \frac{\langle \nabla \varphi(x), h \rangle^n}{n})}  \\
    \Rightarrow \ &\textcolor{blue}{\langle \nabla \varphi(x), h \rangle} + \frac{1}{2} \langle\nabla^2 \varphi(x) h, h \rangle + o(||h||^2)  \geq \textcolor{blue}{\langle \nabla \varphi(x), h \rangle} + \sum_{n=2}^{\infty} \frac{\langle \nabla \varphi(x), h \rangle^n}{n}  \\
    \Rightarrow \ &\frac{1}{2} \langle\nabla^2 \varphi(x) h, h \rangle + o(||h||^2)  \geq \sum_{n=2}^{\infty} \frac{\langle \nabla \varphi(x), h \rangle^n}{n}  \\
    \Rightarrow \ &\frac{1}{2} \langle\nabla^2 \varphi(x) h, h \rangle + o(||h||^2)  \geq \frac{\langle \nabla \varphi(x), h \rangle^2}{2} + \frac{\langle \nabla \varphi(x), h \rangle^3}{3} + \cdots \tag{*}\\
\end{align*}

Examine the terms on the right hand side by Cauchy-Schwarz inequality:

\begin{align*}
    \frac{(\textcolor{orange}{\langle \nabla \varphi(x), h \rangle})^3}{3} 
    &\textcolor{orange}{\leq} \frac{(\textcolor{orange}{||\nabla \varphi(x)|| \cdot ||h||})^3}{3} \\
\end{align*}

Since $||h|| \rightarrow 0$ by our assumption, we can write:

\begin{align*}
    \frac{\langle \nabla \varphi(x), h \rangle^2}{2} + \frac{\langle \nabla \varphi(x), h \rangle^3}{3} + \cdots = o(||h||^2)
\end{align*}

Substituting this bound back into $(*)$, we have:

\begin{align*}
    &\frac{1}{2} \langle\nabla^2 \varphi(x) h, h \rangle + o(||h||^2)  \geq \frac{\langle \nabla \varphi(x), h \rangle^2}{2} + o(||h||^2) \\
    \Rightarrow \ & \ \frac{1}{2} \langle\nabla^2 \varphi(x) h, h \rangle  \geq \frac{\langle \nabla \varphi(x), h \rangle^2}{2} \\
    \Rightarrow \ & \ \langle\nabla^2 \varphi(x) h, h \rangle  \geq \langle \nabla \varphi(x), h \rangle^2 \\
    \Rightarrow \ & \ (\nabla^2 \varphi(x) h)^T h \geq (\nabla \varphi(x)^T h)^T (\nabla \varphi(x)^T h) \\
    \Rightarrow \ & \ h^T \textcolor{blue}{(\nabla^2 \varphi(x))^T} h \geq h^T \textcolor{blue}{\nabla \varphi(x) (\nabla \varphi(x))^T} h \\
    \Rightarrow \ & \ h^T \textcolor{blue}{((\nabla^2 \varphi(x))^T - \nabla \varphi(x) (\nabla \varphi(x))^T)} h \geq 0 \\
    \Rightarrow \ & \ \nabla^2 \varphi(x) - \nabla \varphi(x) (\nabla \varphi(x))^T \succeq 0 \qquad \qquad \text{(since the Hessian is symmetric)}
\end{align*}

Thus, we have proved that:

\begin{align*}
    \nabla^2 \varphi(x) \geq \nabla \varphi(x) (\nabla \varphi(x))^T, \quad \forall x,y \in \domain \varphi
\end{align*}

\textbf{\underline{(3) $\implies$ (1)}}
\bigskip

Suppose $(3)$ holds, so we have:

\begin{align*}
    \nabla^2 \varphi(x) \geq \nabla \varphi(x) (\nabla \varphi(x))^T, \quad \forall x,y \in \domain \varphi
\end{align*}

Since we need to show that $e^{-\varphi(x)}$ is concave, similar to the previous proof, we can define $g(x) = -f(x) = -e^{-\varphi(x)} \ (\text{ where } f(x) = e^{-\varphi(x)})$,
and show that $g(x)$ is convex.
\bigskip

By theorem
\footnote{Y. Nesterov, \textit{Introductory Lectures on Convex Optimization: A Basic Course}, 1st ed., Springer, New York, NY, 2004, p.~55.}, 
we have:
\begin{tcolorbox}[greenbox, title = Theorem 2.1.4]
    Two times continuously differentiable function $f \in \mathcal{F}^2(\mathbb{R}^n)$ iff for any $x \in \mathbb{R}^n$, we have:
    \begin{align*}
        f''(x) \succeq 0
    \end{align*}
\end{tcolorbox}

Therefore, we need to show that $\nabla^2 g(x) \succeq 0$.
We derive the following using the Scalar-by-vector identity
\footnote{``Matrix calculus'', Wikipedia, \url{https://en.wikipedia.org/wiki/Matrix_calculus}}:
\bigskip

If $u = u(x)$ and $v = v(x)$ are vector functions of $x$, then:
\begin{align*}
    \nabla (u \cdot v) = (\nabla u) v^T + u^T (\nabla v)
\end{align*}

Hence, we have:

\begin{align*}
    \nabla^2 g(x) 
    &= \nabla(e^{-\varphi(x)} \nabla \varphi(x)) \\
    &= \left[ \frac{d}{dx}(e^{-\varphi(x)}) \right] (\nabla \varphi(x))^T + e^{-\varphi(x)} \nabla^2 \varphi(x) \\
    &= \textcolor{orange}{-} e^{-\varphi(x)} \textcolor{orange}{(\nabla \varphi(x))(\nabla \varphi(x))^T} + e^{-\varphi(x)} \textcolor{orange}{\nabla^2 \varphi(x)} \\
    &= e^{-\varphi(x)} \left[ \textcolor{orange}{\nabla^2 \varphi(x) - (\nabla \varphi(x))(\nabla \varphi(x))^T} \right]
\end{align*}

By our assumption, we knew that $\nabla^2 \varphi(x) - \nabla \varphi(x) (\nabla \varphi(x))^T \succeq 0$,
and multiplying by $e^{-\varphi(x)} > 0$ would not change the sign, therefore we have:

\begin{align*}
    \nabla^2 g(x) \succeq 0
\end{align*}

And the equivalence of the three statements is proved. $\qquad \square$

\end{document}